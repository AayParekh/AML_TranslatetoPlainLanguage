{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXsIyWeIOC_a",
        "outputId": "8f4a8ec1-a66b-4f5a-ed8d-ae8c2ce2755a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy4Xryf7ODxm",
        "outputId": "62f147a4-3747-494f-f344-466e9a52d880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers version: 4.57.3\n",
            "Datasets version: 4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \"transformers>=4.45.0\" \"datasets>=2.20.0\" accelerate sentencepiece\n",
        "\n",
        "import transformers, datasets\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "print(\"Datasets version:\", datasets.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK1FW5zcW-AT"
      },
      "source": [
        "# 0. Import & Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17u-TC6vcc_S"
      },
      "source": [
        "## Config\n",
        "\n",
        "- `config.dropout = 0.1`\n",
        "  - general Transformer dropout\n",
        "  - Prevents overfitting, smaller more likely to memorize and over fit\n",
        "\n",
        "- `config.attention_dropout = 0.1`\n",
        "  - Attention weights dropout\n",
        "  - Robust, distributed attention, Prevents from focusing too much on single token, (try lower than dropout, or even 0.0?)\n",
        "\n",
        "- `config.activation_dropout = 0.0`\n",
        "  - Feed-forward MLP dropout\n",
        "  - Extra FFN regularization (not necessary bc FFNs already have large dim)\n",
        "\n",
        "Training recipe upgrades\n",
        "- Learning-rate schedule + warmup\n",
        "  Add linear warmup (e.g., 5\u201310% of steps) + linear decay. This often stabilizes seq2seq fine-tuning more than changing dropout.\n",
        "    - warmup_ratio: first % of training steps, learning rate increases linearly from 0, prevent loss spiking early\n",
        "    - lr_scheduler_type: after warmup, lr decreases linearly to 0 over training, more conservative than cosine\n",
        "- Weight decay + label smoothing\n",
        "  Weight decay (e.g., 0.01) + label smoothing (e.g., 0.1) can reduce overfitting and improve fluency.\n",
        "    - label_smoothing_factor: improves generalization, encourages paraphrasing instead of copying\n",
        "    - weight_decay: reduces copying the input verbatim, encourages smoother paraphrases\n",
        "\n",
        "Decoding improvements (huge for perceived quality)\n",
        "- try num_beams=4\n",
        "    - compare 4 sentences choose the best probability sentence\n",
        "- add length_penalty (e.g., 1.0\u20131.2)\n",
        "    - prevent model from cutting too much, longer outputs\n",
        "- add no_repeat_ngram_size=3\n",
        "    - forbids repeating any 3-word sequence\n",
        "- cap with max_new_tokens\n",
        "    - 64 tokens = 3-4 sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07-NCAlLR6IW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Dict, Any\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "# Metrics\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import cmudict\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('corpora/cmudict')\n",
        "except LookupError:\n",
        "    nltk.download('cmudict')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR4ML6aDSy3S"
      },
      "outputs": [],
      "source": [
        "# Paths to JSON files\n",
        "TRAIN_JSON_1 = \"/content/drive/MyDrive/AML_Final_Project/Data/synthetic_train.json\"\n",
        "TRAIN_JSON_2 = \"/content/drive/MyDrive/AML_Final_Project/Data/asset_train.json\"\n",
        "\n",
        "# TEST_JSON  = \"/content/drive/MyDrive/AML_Final_Project/Data/synthetic_test.json\"\n",
        "TEST_JSON  = \"/content/drive/MyDrive/AML_Final_Project/Data/asset_test.json\"\n",
        "\n",
        "MODEL_NAME = \"facebook/bart-base\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/AML_Final_Project/bart-config\"\n",
        "\n",
        "MAX_SOURCE_LENGTH = 128   # input: prompt + original, 80~100 words, 1024 limit\n",
        "MAX_TARGET_LENGTH = 64    # output: simplified, 40~50 words\n",
        "\n",
        "# Training hyperparameters\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 3e-5\n",
        "GRAD_CLIP = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCqOpfHKHs2j"
      },
      "outputs": [],
      "source": [
        "TASK_PREFIX = ()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bELuTdXXCNV"
      },
      "source": [
        "# 1. GPU Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu98OumxR_id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41512246-05d0-4802-a701-9d26c01d4374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y15nWXz_XEqo"
      },
      "source": [
        "# 2. Load Json Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRntN-OfS2GB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215,
          "referenced_widgets": [
            "bdb3e5ea1fd146b885117175ed0a5574",
            "f894485a6d994830bf99bfeaa63ee5f2",
            "b5b59fab464e448fb8ff72cc44e6bb7e",
            "7a851b5d795b430988f2d6822dac28ae",
            "71afa289cec14c48a2ef35be96e75cf4",
            "aa1e9537c6ef4a6497de54c97469a97e",
            "ed657cbdd2ae404e81c330c91e13906f",
            "e10ff089dffa4591bd5a2bacb94e035d",
            "da161e520913433cab271fcb250c273e",
            "99f5f628e19945da81f82fca2db775ed",
            "f447e1a6b22a422dbc948ece14fd6090",
            "71c47b815f634c239dd0acf75bcd28f8",
            "ed9e13f4554042019d5e25c3405a02b7",
            "d84d5dde0cb54ebb93e90e24be782e06",
            "2dc25f947de745e5b0607e3de2119aed",
            "bdf9ae5b4aa6457a8a6c86240158fd23",
            "ea898ddb5f70460e9a74aab37a0e0765",
            "f08015d16d204cc19cb6d7dea66aef58",
            "de07d715d9b346b6894111b5664e2cef",
            "d600e4c4e76b4d90bb350c906a6fb720",
            "8ce18f333a5647368b7f69c439465b07",
            "c77282cb34ee416c805eec58b9a40dd1"
          ]
        },
        "outputId": "82b30e15-7f94-405c-c56b-8204c8a90ad5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bdb3e5ea1fd146b885117175ed0a5574"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating eval split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71c47b815f634c239dd0acf75bcd28f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw train entries: 27330\n",
            "Raw eval entries: 359\n"
          ]
        }
      ],
      "source": [
        "raw_datasets = load_dataset(\n",
        "    \"json\",\n",
        "    data_files={\n",
        "        \"train\": [TRAIN_JSON_1, TRAIN_JSON_2],\n",
        "        \"eval\": TEST_JSON,\n",
        "    },\n",
        ")\n",
        "\n",
        "train_dataset = raw_datasets[\"train\"]\n",
        "eval_dataset = raw_datasets[\"eval\"]\n",
        "\n",
        "print(\"Raw train entries:\", len(train_dataset))\n",
        "print(\"Raw eval entries:\", len(eval_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--ebPlyNXPAm"
      },
      "source": [
        "# 3. Flatten Multiple Simplifications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215,
          "referenced_widgets": [
            "19d7be3b46ed49b4ac65157764742cc5",
            "063c2dd564b848209e7280613ddbd617",
            "9153a4409a824e4ea20f153e1e9b6697",
            "11da22376f6f48e387214c96f94fd2dc",
            "1afa85120f3748d681ac5a44f1fb7cba",
            "7b708acd3f574316adef0bf1333a86f4",
            "0de5b3f5c7764576adad11f1eb82a879",
            "16b80d7b0b674f059faece47cbb316de",
            "54bdbbcceced41c1ac8759d6eb466fb7",
            "c4d8bbc2eb014df7babb9cd30dd9dbb6",
            "eb4f369f74dc4a06a44dc4bf02b8de0f",
            "c25697e8f6094bf0b6fb9d5348af4cab",
            "1144f507f01c41b78ae9e340033ff104",
            "a738afd668c54135b21dfd97ac11b352",
            "867e03654a8443d2989e482f925258ef",
            "57ae63502f3047bda1266e27ff3eba9d",
            "a863e111105b4721a8551f29bad9ebf4",
            "534482f724c5452eb32a95801b4d2733",
            "e59b5b8b09264ae9bcf865da5d0ff909",
            "15c58c4681494890a0d29abe2e4380fb",
            "f7e058b529ac45329e5b5df1b20f54a5",
            "10858ed7e33f43169d6c6fb4e74a38a1"
          ]
        },
        "id": "jDW8PdQZS6rP",
        "outputId": "9ccf8333-50d5-4456-e52d-a97aff7724b7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/27330 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19d7be3b46ed49b4ac65157764742cc5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/359 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c25697e8f6094bf0b6fb9d5348af4cab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset train examples: 45330\n",
            "Subset eval examples: 3590\n"
          ]
        }
      ],
      "source": [
        "# Turn:\n",
        "#   original = \"...\"\n",
        "#   simplifications = [\"a\", \"b\", \"c\"]\n",
        "# into:\n",
        "#   (original, \"a\"), (original, \"b\"), (original, \"c\")\n",
        "\n",
        "def explode_simplifications(examples: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    new_originals = []\n",
        "    new_simplified = []\n",
        "\n",
        "    for orig, sims in zip(examples[\"original\"], examples[\"simplifications\"]):\n",
        "        for s in sims:\n",
        "            new_originals.append(orig)\n",
        "            new_simplified.append(s)\n",
        "\n",
        "    return {\n",
        "        \"original\": new_originals,\n",
        "        \"simplified\": new_simplified,\n",
        "    }\n",
        "\n",
        "train_flat = train_dataset.map(\n",
        "    explode_simplifications,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        ")\n",
        "\n",
        "eval_flat = eval_dataset.map(\n",
        "    explode_simplifications,\n",
        "    batched=True,\n",
        "    remove_columns=eval_dataset.column_names,\n",
        ")\n",
        "\n",
        "print(\"Subset train examples:\", len(train_flat))\n",
        "print(\"Subset eval examples:\", len(eval_flat))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB_cTpnTXkrg"
      },
      "source": [
        "# 4. Tokenizer & Model (BART)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6c34803260724a73ac7eebdf5604d305",
            "4b8a915140ea4bb8bc4e9919ad61332b",
            "6780c40069d74ec4828574ceceacee21",
            "fa9c66a293fe43e397eff0fa88d2e5ac",
            "c4f8c0ba860f4e33a3e73d4d9622b492",
            "5e94a76bb5d64722affbfb255bceea71",
            "62a673308c054517a6e9c61d7fa99240",
            "baffbbcb7266431c9665de28e8cf823c",
            "e5542c34cee1446bb818d08079f2758e",
            "c7460dcd3b674ad195bb6c52c26972b3",
            "285d76ed815d424ca67439102dd4ea95",
            "49c9268a3a734576bc362ad55ead0b53",
            "3fc69a53110e42dba051293898b89900",
            "8a2d3f4e5bba44fb940290a0ecc016ad",
            "ec6436f0a2ab4c8e84ef41ad32e7d385",
            "dc33d2ccf36e4b9989bb1e4afb454b70",
            "a18209775c9c461cb47c18d00057b8ca",
            "78b137f121d74463a844133f540caf5a",
            "5c9d6af0cc8047c4982491dce66094f6",
            "b7d64bc5a48149fcbf8593412fa8dca8",
            "eef42283be09491ca88edc0aab767c86",
            "e874e3badbf9463d9fff0a31749dd595",
            "b77bcc48a97a413ab5b090e8e8744598",
            "bc111e7a17f64ced8dfb3efb48780ea7",
            "01153aa65b394a4e8f98fb7c5761f1d0",
            "b03146ba73c54536911f70203af2ee27",
            "507b6a1fcc2344409bceb00a62b54980",
            "3c3499a11007416a817c933cbfcee5ec",
            "33a1990b7f0a4a21bb25a300d4f7d7c6",
            "899c2f0963c1409b85aa24f812aa2e5f",
            "a643368e35a643f3b0d35d1bc5dca16e",
            "ec6b32cd97d04a0e88553d24aa5f2ec9",
            "e7c59dc6b6b94e31856a09c648c17aa8",
            "6bf86e81debb496fb9a7cada28c97809",
            "261d02c6aa6740da9b28e0af2a4f4906",
            "d49e24d8b3dd40f4aace919eda7730ac",
            "87541758e90b4b2189a610a81700404c",
            "1d25b721fff543d0a6d7781b83ddc9c1",
            "552583d3aea243ddb4b1a7e601249c1d",
            "df705c28d124468cb3e42a7f34714dce",
            "27eb834b45894db89320d7bffd98faeb",
            "cae64bf4200945b7aaf29b5a914485f9",
            "a4b755d676ad41fabbef83a12166cd35",
            "a638a3810fa844f68eb8a960fa5d3742",
            "2dd466ddba784263b60cb66b3493ab02",
            "c42b7a26ef4f42e38c82b0c9b4a5c25c",
            "db80c07594ed40c480151f6fd685cf47",
            "bd999e6357fd423db74e587570f588a2",
            "f4ac0317aae54eeeba73ce14d35f8557",
            "d6056bb690074d23a932bba0142cdd24",
            "3ff94c56c9d54ad9b6512e44a35e876d",
            "9fe3e441f49848a78ae2c0431647ab93",
            "bdbbe3b9fe80450184758f6e3b5e4b58",
            "1cb03416723c4d2d839cac177810bc8e",
            "e71d28d22cdc4b5cbf46187f92b22546"
          ]
        },
        "collapsed": true,
        "id": "FCXP8rNaS_1u",
        "outputId": "1406b406-e360-44a4-8566-e266e4189b5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c34803260724a73ac7eebdf5604d305"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49c9268a3a734576bc362ad55ead0b53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b77bcc48a97a413ab5b090e8e8744598"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bf86e81debb496fb9a7cada28c97809"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2dd466ddba784263b60cb66b3493ab02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "config.use_cache = False\n",
        "\n",
        "# Config\n",
        "config.dropout = 0.1\n",
        "config.attention_dropout = 0.1\n",
        "config.activation_dropout = 0.0\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, config=config)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEleY0sJXvmN"
      },
      "source": [
        "# 5. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "6e9c9394c943459ead98965f4eed4e3a",
            "dd90272833c8481c844f622164b0d324",
            "a4fda30955e945ea9eeaeae2728b78ae",
            "a892b606bb82492db13eb71a6b0d0dc4",
            "a46c93cd1134485088be7e08d70e9235",
            "4ea3fad5ff8b4fbf9e6258893b4a4285",
            "32c87ad6cbd5464fb5c5c3d193f610bc",
            "1c42117aaf884c77b928b0c3eaa76dbc",
            "99cd3edcd186499ab6b367534539e07e",
            "68569b7f2dac4760bb3f825befc5ac7e",
            "056154cd6faf45e2890176a631c3865f",
            "ff45b74e01b649a182a5f47e9ed6a144",
            "b0f2f443c3cb4ebba2fdc8b2f98c7032",
            "1597587db2104a7e973eea5951a09817",
            "1b68445c7b4c4070a387b60a5c7cb553",
            "fd9cf8dcc9fc43c2bec34961d9caf6d5",
            "324e6df267f54a6b989be3545c2a3eac",
            "19109cd18a1d4b49bd7a79ae3c2ce500",
            "69aac1193293494982ecf42c0db7494f",
            "969e1de3ec654882a122cda06aefdd8a",
            "7d2e079d2e2e4d32ad277799acc9973c",
            "72ce1780f70044199dcca0102478cbda"
          ]
        },
        "id": "wF6mPtEvYNj3",
        "outputId": "694d46a5-93f2-4e86-b416-f674af379c8e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/45330 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e9c9394c943459ead98965f4eed4e3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3590 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff45b74e01b649a182a5f47e9ed6a144"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized train sample keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
          ]
        }
      ],
      "source": [
        "def preprocess_function(examples: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    # Build source strings with the task prefix + original text\n",
        "    sources = [\n",
        "        text.strip() for text in examples[\"original\"]\n",
        "    ]\n",
        "    targets = [text.strip() for text in examples[\"simplified\"]]\n",
        "\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(\n",
        "        sources,\n",
        "        truncation=True,\n",
        "        padding=True, # \"max_length\" for fixed shapes\n",
        "        # max_length=MAX_SOURCE_LENGTH,\n",
        "    )\n",
        "\n",
        "    labels = tokenizer(\n",
        "        text_target=targets,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        # max_length=MAX_TARGET_LENGTH,\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_train = train_flat.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_flat.column_names,\n",
        ")\n",
        "\n",
        "tokenized_eval = eval_flat.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=eval_flat.column_names,\n",
        ")\n",
        "\n",
        "print(\"Tokenized train sample keys:\", tokenized_train[0].keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T39FruwYOxO"
      },
      "source": [
        "# 6. Dataloaders & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW3s3PIJYXXO"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    # max_length=MAX_SOURCE_LENGTH,\n",
        "    label_pad_token_id=-100,\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_train,\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_eval,\n",
        "    shuffle=False,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE,\n",
        "                              weight_decay=0.01)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nZR-fNhpnAD"
      },
      "source": [
        "# 7. Training (seq2seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wMKojLYXuQi"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "\n",
        "    label_smoothing_factor=0.1, # label smoothing\n",
        "    warmup_ratio=0.1, # warm up\n",
        "    lr_scheduler_type = \"linear\", # LR scheduler\n",
        "\n",
        "    predict_with_generate=False,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "\n",
        "    report_to=\"none\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyQ2BnNZXwKu"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "ESAWF9X6X8qs",
        "outputId": "d9f3ce42-72b5-4071-c5c2-3239da37a6e8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40051' max='56670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40051/56670 1:17:50 < 32:18, 8.57 it/s, Epoch 7.07/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.538100</td>\n",
              "      <td>1.724008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.706200</td>\n",
              "      <td>1.670887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.659700</td>\n",
              "      <td>1.651295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.632600</td>\n",
              "      <td>1.640602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.613600</td>\n",
              "      <td>1.636300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.598800</td>\n",
              "      <td>1.632226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.586100</td>\n",
              "      <td>1.632363</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV8TOQ9hX_OP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logs = trainer.state.log_history\n",
        "\n",
        "train_loss = [x[\"loss\"] for x in logs if \"loss\" in x and \"epoch\" in x]\n",
        "train_ep   = [x[\"epoch\"] for x in logs if \"loss\" in x and \"epoch\" in x]\n",
        "\n",
        "eval_loss  = [x[\"eval_loss\"] for x in logs if \"eval_loss\" in x]\n",
        "eval_ep    = [x[\"epoch\"] for x in logs if \"eval_loss\" in x]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(train_ep, train_loss, marker=\"o\", label=\"Train loss\")\n",
        "plt.plot(eval_ep,  eval_loss,  marker=\"o\", label=\"Eval loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train vs Eval Loss\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Training complete. Model saved to:\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "UwOi3YImX8UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrZ1-Nb_ZGUQ"
      },
      "source": [
        "# 8. Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seq2seq\n",
        "FINAL_CKPT = \"/content/drive/MyDrive/AML_Final_Project/bart-config/checkpoint-final\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(FINAL_CKPT)\n",
        "inference_model = AutoModelForSeq2SeqLM.from_pretrained(FINAL_CKPT).to(device)\n",
        "\n",
        "inference_model.eval()\n",
        "inference_model.config.use_cache = True\n"
      ],
      "metadata": {
        "id": "iOSOAavdzsTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y672SFTGO6y"
      },
      "outputs": [],
      "source": [
        "gen_config = GenerationConfig.from_model_config(inference_model.config)\n",
        "\n",
        "gen_config.use_cache = False\n",
        "gen_config.num_beams = 4\n",
        "gen_config.max_new_tokens = 64\n",
        "gen_config.early_stopping = True\n",
        "gen_config.forced_bos_token_id = 0\n",
        "gen_config.no_repeat_ngram_size = 3\n",
        "gen_config.length_penalty = 1.1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPEy2A61Us3F"
      },
      "outputs": [],
      "source": [
        "def simplify_trained(text: str, max_new_tokens: int = 64, num_beams: int = 4) -> str:\n",
        "    model_input = text.strip()\n",
        "\n",
        "    enc = tokenizer(\n",
        "        model_input,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_SOURCE_LENGTH,\n",
        "    )\n",
        "    enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = inference_model.generate(\n",
        "            **enc,\n",
        "            generation_config=gen_config\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True) # seq2seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoLlXBJhZ3Hk",
        "outputId": "15e92637-7541-45f9-87e5-5ac18dd6e8de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ORIGINAL ===\n",
            "\n",
            "Jeddah is the principal gateway to Mecca, Islam's holiest city, which able-bodied Muslims are required to visit at least once in their lifetime.\n",
            "\n",
            "=== SIMPLIFIED (MODEL OUTPUT) ===\n",
            "\n",
            "Jossah is the main entry point for Muslims to visit in their lifetime. It is also the home of Islam's most famous city.\n"
          ]
        }
      ],
      "source": [
        "test_text = (\n",
        "    # \"Adjacent counties are Marin (to the south), Mendocino (to the north), \"\n",
        "    # \"Lake (northeast), Napa (to the east), and Solano and Contra Costa (to the southeast).\"\n",
        "    \"Jeddah is the principal gateway to Mecca, Islam's holiest city, \"\n",
        "    \"which able-bodied Muslims are required to visit at least once in their lifetime.\"\n",
        "    )\n",
        "\n",
        "print(\"\\n=== ORIGINAL ===\\n\")\n",
        "print(test_text)\n",
        "\n",
        "print(\"\\n=== SIMPLIFIED (MODEL OUTPUT) ===\\n\")\n",
        "print(simplify_trained(test_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co-1XgNTxJ-R"
      },
      "source": [
        "# 9. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-myqIZOD6h-D"
      },
      "source": [
        "## 9-1. Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5LpGWdZ5PF_"
      },
      "outputs": [],
      "source": [
        "# Initialize CMU Pronouncing Dictionary for syllable counting\n",
        "d = cmudict.dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAOfBZcC6kW6"
      },
      "outputs": [],
      "source": [
        "def count_syllables(word):\n",
        "    \"\"\"Count syllables in a word using CMU Pronouncing Dictionary.\"\"\"\n",
        "    word = word.lower()\n",
        "    if word in d:\n",
        "        return max([len([y for y in x if y[-1].isdigit()]) for x in d[word]])\n",
        "    else:\n",
        "        # Fallback: estimate syllables by counting vowel groups\n",
        "        word = word.lower()\n",
        "        count = 0\n",
        "        vowels = 'aeiouy'\n",
        "        previous_was_vowel = False\n",
        "        for char in word:\n",
        "            is_vowel = char in vowels\n",
        "            if is_vowel and not previous_was_vowel:\n",
        "                count += 1\n",
        "            previous_was_vowel = is_vowel\n",
        "        # Adjust for silent 'e'\n",
        "        if word.endswith('e'):\n",
        "            count -= 1\n",
        "        if count == 0:\n",
        "            count = 1\n",
        "        return count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZdLr34Z6l5H"
      },
      "outputs": [],
      "source": [
        "def flesch_kincaid_grade(text):\n",
        "    \"\"\"\n",
        "    Calculate Flesch-Kincaid Grade Level.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to analyze\n",
        "\n",
        "    Returns:\n",
        "        float: The grade level score\n",
        "    \"\"\"\n",
        "    # Split into sentences\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    # Split into words\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "    if not sentences or not words:\n",
        "        return 0.0\n",
        "\n",
        "    total_sentences = len(sentences)\n",
        "    total_words = len(words)\n",
        "    total_syllables = sum(count_syllables(word) for word in words)\n",
        "\n",
        "    # Flesch-Kincaid Grade Level formula\n",
        "    # https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests\n",
        "    grade = 0.39 * (total_words / total_sentences) + 11.8 * (total_syllables / total_words) - 15.59\n",
        "\n",
        "    return round(grade, 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qI0xCO-N6pQx"
      },
      "outputs": [],
      "source": [
        "def bleu_score(reference, candidate):\n",
        "    \"\"\"\n",
        "    Calculate BLEU score between reference and candidate text.\n",
        "\n",
        "    BLEU measures how similar the candidate text is to the reference text.\n",
        "    Score ranges from 0 to 1, where 1 indicates perfect match.\n",
        "\n",
        "    Args:\n",
        "        reference (str): The reference (original) text\n",
        "        candidate (str): The candidate (simplified) text\n",
        "\n",
        "    Returns:\n",
        "        float: BLEU score between 0 and 1\n",
        "    \"\"\"\n",
        "    # Tokenize texts into words\n",
        "    reference_tokens = re.findall(r'\\b\\w+\\b', reference.lower())\n",
        "    candidate_tokens = re.findall(r'\\b\\w+\\b', candidate.lower())\n",
        "\n",
        "    # BLEU expects reference as list of lists\n",
        "    reference_list = [reference_tokens]\n",
        "\n",
        "    # Use smoothing to handle cases with no n-gram matches\n",
        "    smoothing = SmoothingFunction().method1\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    score = sentence_bleu(reference_list, candidate_tokens, smoothing_function=smoothing)\n",
        "\n",
        "    return round(score, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY4wXHrU6rfc"
      },
      "outputs": [],
      "source": [
        "def sari_score(source, reference, candidate):\n",
        "    \"\"\"\n",
        "    Calculate SARI score for text simplification.\n",
        "\n",
        "    SARI measures the quality of simplification by evaluating:\n",
        "    - Added words (should be simple/appropriate)\n",
        "    - Deleted words (should remove complex content)\n",
        "    - Kept words (should retain important information)\n",
        "\n",
        "    Args:\n",
        "        source (str): The original source text\n",
        "        reference (str): The reference simplified text\n",
        "        candidate (str): The candidate simplified text\n",
        "\n",
        "    Returns:\n",
        "        float: SARI score (0-100 scale)\n",
        "    \"\"\"\n",
        "    # Tokenize texts\n",
        "    source_tokens = set(re.findall(r'\\b\\w+\\b', source.lower()))\n",
        "    reference_tokens = set(re.findall(r'\\b\\w+\\b', reference.lower()))\n",
        "    candidate_tokens = set(re.findall(r'\\b\\w+\\b', candidate.lower()))\n",
        "\n",
        "    # Calculate add, keep, and delete operations\n",
        "    # Add: words in candidate but not in source\n",
        "    added = candidate_tokens - source_tokens\n",
        "    # Keep: words in both source and candidate\n",
        "    kept = source_tokens & candidate_tokens\n",
        "    # Delete: words in source but not in candidate\n",
        "    deleted = source_tokens - candidate_tokens\n",
        "\n",
        "    # Calculate precision and recall for each operation\n",
        "    # Add score: precision of added words (how many added words are in reference)\n",
        "    if added:\n",
        "        add_precision = len(added & reference_tokens) / len(added)\n",
        "    else:\n",
        "        add_precision = 0.0\n",
        "\n",
        "    # Keep score: F1 of kept words\n",
        "    if kept or (source_tokens & reference_tokens):\n",
        "        keep_precision = len(kept & reference_tokens) / len(kept) if kept else 0\n",
        "        keep_recall = len(kept & reference_tokens) / len(source_tokens & reference_tokens) if (source_tokens & reference_tokens) else 0\n",
        "        if keep_precision + keep_recall > 0:\n",
        "            keep_f1 = 2 * keep_precision * keep_recall / (keep_precision + keep_recall)\n",
        "        else:\n",
        "            keep_f1 = 0\n",
        "    else:\n",
        "        keep_f1 = 0\n",
        "\n",
        "    # Delete score: precision of deleted words (how many deleted words are also deleted in reference)\n",
        "    reference_deleted = source_tokens - reference_tokens\n",
        "    if deleted:\n",
        "        delete_precision = len(deleted & reference_deleted) / len(deleted)\n",
        "    else:\n",
        "        delete_precision = 0.0\n",
        "\n",
        "    # SARI is the average of the three scores (scaled to 0-100)\n",
        "    sari = (add_precision + keep_f1 + delete_precision) / 3 * 100\n",
        "\n",
        "    return round(sari, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iiuAWXc6uuC"
      },
      "outputs": [],
      "source": [
        "def compression_ratio(original, simplified):\n",
        "    \"\"\"\n",
        "    Calculate compression ratio between original and simplified text.\n",
        "\n",
        "    Measures how much shorter the simplified text is compared to the original.\n",
        "    Typical good simplifications: 0.6-0.8 (20-40% shorter)\n",
        "\n",
        "    Args:\n",
        "        original (str): The original text\n",
        "        simplified (str): The simplified text\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with character-based and word-based compression ratios\n",
        "    \"\"\"\n",
        "    # Character-based compression\n",
        "    char_ratio = len(simplified) / len(original) if len(original) > 0 else 0\n",
        "\n",
        "    # Word-based compression\n",
        "    original_words = len(re.findall(r'\\b\\w+\\b', original))\n",
        "    simplified_words = len(re.findall(r'\\b\\w+\\b', simplified))\n",
        "    word_ratio = simplified_words / original_words if original_words > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'char_ratio': round(char_ratio, 4),\n",
        "        'word_ratio': round(word_ratio, 4),\n",
        "        'char_reduction_pct': round((1 - char_ratio) * 100, 2),\n",
        "        'word_reduction_pct': round((1 - word_ratio) * 100, 2)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzeN3nic6w8R"
      },
      "outputs": [],
      "source": [
        "def average_sentence_length(text):\n",
        "    \"\"\"\n",
        "    Calculate average sentence length in words.\n",
        "\n",
        "    Simpler text typically has shorter sentences.\n",
        "    General guidelines:\n",
        "    - <15 words: Very easy\n",
        "    - 15-20 words: Easy\n",
        "    - 20-25 words: Moderate\n",
        "    - >25 words: Difficult\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to analyze\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with average sentence length and total sentences\n",
        "    \"\"\"\n",
        "    # Split into sentences\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    if not sentences:\n",
        "        return {\n",
        "            'avg_sentence_length': 0.0,\n",
        "            'total_sentences': 0\n",
        "        }\n",
        "\n",
        "    # Count words in each sentence\n",
        "    total_words = 0\n",
        "    for sentence in sentences:\n",
        "        words = re.findall(r'\\b\\w+\\b', sentence)\n",
        "        total_words += len(words)\n",
        "\n",
        "    avg_length = total_words / len(sentences)\n",
        "\n",
        "    return {\n",
        "        'avg_sentence_length': round(avg_length, 2),\n",
        "        'total_sentences': len(sentences)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMZSCVCw7WXx"
      },
      "outputs": [],
      "source": [
        "def print_results(results):\n",
        "    \"\"\"Pretty print the aggregated results.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"DATASET METRICS (n={results['dataset_size']} pairs)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\n\ud83d\udcca READABILITY (Flesch-Kincaid Grade Level)\")\n",
        "    print(f\"  Original:    {results['flesch_kincaid']['original']['mean']:.2f} \u00b1 {results['flesch_kincaid']['original']['std']:.2f}\")\n",
        "    print(f\"  Simplified:  {results['flesch_kincaid']['simplified']['mean']:.2f} \u00b1 {results['flesch_kincaid']['simplified']['std']:.2f}\")\n",
        "    print(f\"  Improvement: {results['flesch_kincaid']['improvement']:.2f} grade levels\")\n",
        "\n",
        "    print(\"\\n\ud83d\udcdd SEMANTIC SIMILARITY (BLEU Score)\")\n",
        "    print(f\"  Mean:   {results['bleu']['mean']:.4f} \u00b1 {results['bleu']['std']:.4f}\")\n",
        "    print(f\"  Median: {results['bleu']['median']:.4f}\")\n",
        "\n",
        "    print(\"\\n\ud83d\udccf COMPRESSION\")\n",
        "    print(f\"  Word Ratio:  {results['compression']['word_ratio']['mean']:.4f} ({results['compression']['avg_word_reduction_pct']:.1f}% reduction)\")\n",
        "    print(f\"  Char Ratio:  {results['compression']['char_ratio']['mean']:.4f}\")\n",
        "\n",
        "    print(\"\\n\ud83d\udcd0 SENTENCE LENGTH (words/sentence)\")\n",
        "    print(f\"  Original:    {results['avg_sentence_length']['original']['mean']:.2f} \u00b1 {results['avg_sentence_length']['original']['std']:.2f}\")\n",
        "    print(f\"  Simplified:  {results['avg_sentence_length']['simplified']['mean']:.2f} \u00b1 {results['avg_sentence_length']['simplified']['std']:.2f}\")\n",
        "    print(f\"  Reduction:   {results['avg_sentence_length']['reduction']:.2f} words\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhwYlobp67Km"
      },
      "source": [
        "## 9-2. Calculate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6ygcGmU5Qcg"
      },
      "outputs": [],
      "source": [
        "def generate_predictions(data, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    for item in tqdm(data, desc=\"Generating predictions\"):\n",
        "        src = item[\"original\"].strip()\n",
        "\n",
        "        enc = tokenizer(\n",
        "            src,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=MAX_SOURCE_LENGTH,\n",
        "        )\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out_ids = model.generate(\n",
        "                **enc,\n",
        "                generation_config=gen_config\n",
        "            )\n",
        "\n",
        "        pred = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "        preds.append(pred)\n",
        "\n",
        "    return preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_azj4h4sxQWK"
      },
      "outputs": [],
      "source": [
        "def calculate_dataset_metrics(data, preds):\n",
        "    fk_original_scores = []\n",
        "    fk_pred_scores = []\n",
        "\n",
        "    bleu_scores = []\n",
        "    sari_scores = []\n",
        "\n",
        "    compression_word_ratios = []\n",
        "    compression_char_ratios = []\n",
        "\n",
        "    asl_original_scores = []\n",
        "    asl_pred_scores = []\n",
        "\n",
        "    for item, pred in tqdm(list(zip(data, preds)), desc=\"Scoring metrics\"):\n",
        "        original = item[\"original\"]\n",
        "        references = item.get(\"simplifications\", [item.get(\"simplified\", \"\")])\n",
        "\n",
        "        # FKGL\n",
        "        fk_original_scores.append(flesch_kincaid_grade(original))\n",
        "        fk_pred_scores.append(flesch_kincaid_grade(pred))\n",
        "\n",
        "        # BLEU: reference(s) vs candidate(pred)\n",
        "        bleu_per_refs = [bleu_score(ref, pred) for ref in references if ref and ref.strip()]\n",
        "        bleu_scores.append(max(bleu_per_refs) if bleu_per_refs else 0.0)\n",
        "\n",
        "        # SARI approximation: average across refs\n",
        "        sari_per_refs = [sari_score(original, ref, pred) for ref in references if ref and ref.strip()]\n",
        "        sari_scores.append(float(np.mean(sari_per_refs)) if sari_per_refs else 0.0)\n",
        "\n",
        "        # Compression\n",
        "        comp = compression_ratio(original, pred)\n",
        "        compression_word_ratios.append(comp[\"word_ratio\"])\n",
        "        compression_char_ratios.append(comp[\"char_ratio\"])\n",
        "\n",
        "        # Avg sentence length\n",
        "        asl_orig = average_sentence_length(original)[\"avg_sentence_length\"]\n",
        "        asl_p = average_sentence_length(pred)[\"avg_sentence_length\"]\n",
        "        asl_original_scores.append(asl_orig)\n",
        "        asl_pred_scores.append(asl_p)\n",
        "\n",
        "    results = {\n",
        "        \"dataset_size\": len(data),\n",
        "\n",
        "        \"flesch_kincaid\": {\n",
        "            \"original\": {\n",
        "                \"mean\": round(float(np.mean(fk_original_scores)), 2),\n",
        "                \"std\": round(float(np.std(fk_original_scores)), 2),\n",
        "                \"median\": round(float(np.median(fk_original_scores)), 2),\n",
        "            },\n",
        "            \"pred\": {\n",
        "                \"mean\": round(float(np.mean(fk_pred_scores)), 2),\n",
        "                \"std\": round(float(np.std(fk_pred_scores)), 2),\n",
        "                \"median\": round(float(np.median(fk_pred_scores)), 2),\n",
        "            },\n",
        "            \"improvement\": round(float(np.mean(fk_original_scores) - np.mean(fk_pred_scores)), 2),\n",
        "        },\n",
        "\n",
        "        \"bleu_max_over_refs\": {\n",
        "            \"mean\": round(float(np.mean(bleu_scores)), 4),\n",
        "            \"std\": round(float(np.std(bleu_scores)), 4),\n",
        "            \"median\": round(float(np.median(bleu_scores)), 4),\n",
        "        },\n",
        "\n",
        "        \"sari\": {\n",
        "            \"mean\": round(np.mean(sari_scores), 2),\n",
        "            \"std\": round(np.std(sari_scores), 2),\n",
        "            \"median\": round(float(np.median(sari_scores)), 2)\n",
        "        },\n",
        "\n",
        "        \"compression\": {\n",
        "            \"word_ratio\": {\n",
        "                \"mean\": round(float(np.mean(compression_word_ratios)), 4),\n",
        "                \"std\": round(float(np.std(compression_word_ratios)), 4),\n",
        "                \"median\": round(float(np.median(compression_word_ratios)), 4),\n",
        "            },\n",
        "            \"char_ratio\": {\n",
        "                \"mean\": round(float(np.mean(compression_char_ratios)), 4),\n",
        "                \"std\": round(float(np.std(compression_char_ratios)), 4),\n",
        "                \"median\": round(float(np.median(compression_char_ratios)), 4),\n",
        "            },\n",
        "            \"avg_word_reduction_pct\": round((1 - float(np.mean(compression_word_ratios))) * 100, 2),\n",
        "        },\n",
        "\n",
        "        \"avg_sentence_length\": {\n",
        "            \"original\": {\n",
        "                \"mean\": round(float(np.mean(asl_original_scores)), 2),\n",
        "                \"std\": round(float(np.std(asl_original_scores)), 2),\n",
        "                \"median\": round(float(np.median(asl_original_scores)), 2),\n",
        "            },\n",
        "            \"pred\": {\n",
        "                \"mean\": round(float(np.mean(asl_pred_scores)), 2),\n",
        "                \"std\": round(float(np.std(asl_pred_scores)), 2),\n",
        "                \"median\": round(float(np.median(asl_pred_scores)), 2),\n",
        "            },\n",
        "            \"reduction\": round(float(np.mean(asl_original_scores) - np.mean(asl_pred_scores)), 2),\n",
        "        },\n",
        "    }\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "8ed71e7b38864b57a76c272319ec5b2d",
            "64718d115fdd41b6922b7e876f8b4d41",
            "0d045493783649c1ab646413bdc89e65",
            "888e6c90f723457fb14342c25d786f89",
            "5669c774ac15456a8c015665574bd1c0",
            "e130504f05294143845f50fffceea8e6",
            "77f3f31d021142a89bad2ef68d926839",
            "23064fffb6b7478f9d7c0f4ab50d618f",
            "0c264834b70b449eab12bbb48044b26f",
            "88e33f9548a64fda9f7a219bd337b226",
            "b1ada6fa631b4c20bcdf4bffa7eb34b2",
            "4612019d234049c8845fadf82c5d0fa7",
            "831f5e07af9746b6abb5a077411279f4",
            "bdf665cd5de14744941f49959548407f",
            "4391fc64a7f240969faf55120326a42e",
            "7446019b117546728985e1d600eb60e5",
            "3169782012724f1ea2afdf990f2b7221",
            "ee0bec220ec34f2b82581e1bd2fb4722",
            "8cfad7f877cd419095d9d3f26fcdd8c1",
            "2fec1dcd2baf407c9eb251ec2b32b320",
            "1d20a5ca31734f9c935480f7aa2753f9",
            "79a01b968ee8437bae42e5289e381e7b"
          ]
        },
        "id": "v8yIjazbxfg4",
        "outputId": "9492454d-744b-42ef-b762-3360f6aff366",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating predictions:   0%|          | 0/359 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ed71e7b38864b57a76c272319ec5b2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Scoring metrics:   0%|          | 0/359 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4612019d234049c8845fadf82c5d0fa7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "with open(TEST_JSON, \"r\") as f:\n",
        "    eval_data = json.load(f)\n",
        "\n",
        "'''\n",
        "random.seed(1)\n",
        "\n",
        "n_total = len(eval_data)\n",
        "n_10 = int(0.1 * n_total)\n",
        "\n",
        "eval_data_10 = random.sample(eval_data, n_10)\n",
        "'''\n",
        "\n",
        "best_model = inference_model\n",
        "\n",
        "preds = generate_predictions(eval_data, best_model, tokenizer, device)\n",
        "\n",
        "results = calculate_dataset_metrics(eval_data, preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpY0m8Ds27Zd",
        "outputId": "43e03f02-e3a2-4409-eed3-24f3c5c70595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dataset_size': 359,\n",
              " 'flesch_kincaid': {'original': {'mean': 11.8, 'std': 3.92, 'median': 11.68},\n",
              "  'pred': {'mean': 8.6, 'std': 3.57, 'median': 8.18},\n",
              "  'improvement': 3.2},\n",
              " 'bleu_max_over_refs': {'mean': 0.5644, 'std': 0.2107, 'median': 0.5703},\n",
              " 'sari': {'mean': np.float64(41.35),\n",
              "  'std': np.float64(11.72),\n",
              "  'median': 39.61},\n",
              " 'compression': {'word_ratio': {'mean': 0.8998,\n",
              "   'std': 0.1712,\n",
              "   'median': 0.9286},\n",
              "  'char_ratio': {'mean': 0.8571, 'std': 0.1645, 'median': 0.887},\n",
              "  'avg_word_reduction_pct': 10.02},\n",
              " 'avg_sentence_length': {'original': {'mean': 19.37,\n",
              "   'std': 8.01,\n",
              "   'median': 19.0},\n",
              "  'pred': {'mean': 14.33, 'std': 6.48, 'median': 12.5},\n",
              "  'reduction': 5.04}}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8bqdvg27ggJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "879b39ee-f331-45d1-8322-6aae666e553b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/AML_Final_Project/Results/config_asset_results.json\n"
          ]
        }
      ],
      "source": [
        "# Save results\n",
        "OUT_PATH = \"/content/drive/MyDrive/AML_Final_Project/Results/config_asset_results.json\"\n",
        "with open(OUT_PATH, \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", OUT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J6qRGo_emH63"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}