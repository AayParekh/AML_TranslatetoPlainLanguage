{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "330eaea4-8f1c-4390-b0ee-1864112da034",
      "metadata": {
        "id": "330eaea4-8f1c-4390-b0ee-1864112da034"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e8a32f9-cb5b-47df-86d5-d668e2e9890f",
      "metadata": {
        "id": "5e8a32f9-cb5b-47df-86d5-d668e2e9890f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Evaluate baseline BART (no training) on:\n",
        "  - FULL asset_test.json\n",
        "  - 10% of synthetic_test.json\n",
        "\n",
        "For each set:\n",
        "  - Input = original sentences ONLY\n",
        "  - Model generates its own simplifications\n",
        "  - We compare model outputs to originals + human simplifications (if present)\n",
        "  - We save JSON with: original, simplifications, bart_baseline, metrics\n",
        "\"\"\"\n",
        "\n",
        "# ============================\n",
        "# 0. SETUP: DRIVE & INSTALLS\n",
        "# ============================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q \"transformers>=4.45.0\" sentencepiece nltk\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import cmudict\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# ============================\n",
        "# 1. MODEL CONFIG (BASELINE BART)\n",
        "# ============================\n",
        "\n",
        "MODEL_NAME = \"facebook/bart-base\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "TASK_PREFIX = (\n",
        "    \"Explain this in simple, plain language for a general audience. \"\n",
        "    \"Use short sentences and everyday words, but keep all important information.\\n\\n\"\n",
        ")\n",
        "\n",
        "MAX_SOURCE_LENGTH = 128\n",
        "MAX_NEW_TOKENS   = 64\n",
        "NUM_BEAMS        = 4\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model     = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
        "model.eval()\n",
        "\n",
        "def simplify_baseline(text: str) -> str:\n",
        "    \"\"\"Run the untrained BART-base as a plain-language simplifier.\"\"\"\n",
        "    model_input = TASK_PREFIX + text.strip()\n",
        "\n",
        "    enc = tokenizer(\n",
        "        model_input,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_SOURCE_LENGTH,\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            num_beams=NUM_BEAMS,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Quick sanity check (optional)\n",
        "test_text = (\n",
        "    \"Adjacent counties are Marin (to the south), Mendocino (to the north), \"\n",
        "    \"Lake (northeast), Napa (to the east), and Solano and Contra Costa (to the southeast).\"\n",
        ")\n",
        "print(\"\\n=== QUICK SANITY CHECK (BASELINE BART) ===\")\n",
        "print(\"ORIGINAL:\\n\", test_text)\n",
        "print(\"\\nBASELINE OUTPUT:\\n\", simplify_baseline(test_text))\n",
        "\n",
        "# ============================\n",
        "# 2. NLTK / CMUDICT SETUP\n",
        "# ============================\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/cmudict')\n",
        "except LookupError:\n",
        "    nltk.download('cmudict')\n",
        "\n",
        "d = cmudict.dict()\n",
        "\n",
        "# ============================\n",
        "# 3. METRIC FUNCTIONS\n",
        "# ============================\n",
        "\n",
        "def count_syllables(word):\n",
        "    word = word.lower()\n",
        "    if word in d:\n",
        "        return max([len([y for y in x if y[-1].isdigit()]) for x in d[word]])\n",
        "    else:\n",
        "        word = word.lower()\n",
        "        count = 0\n",
        "        vowels = 'aeiouy'\n",
        "        previous_was_vowel = False\n",
        "        for char in word:\n",
        "            is_vowel = char in vowels\n",
        "            if is_vowel and not previous_was_vowel:\n",
        "                count += 1\n",
        "            previous_was_vowel = is_vowel\n",
        "        if word.endswith('e'):\n",
        "            count -= 1\n",
        "        if count == 0:\n",
        "            count = 1\n",
        "        return count\n",
        "\n",
        "def flesch_kincaid_grade(text):\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "    if not sentences or not words:\n",
        "        return 0.0\n",
        "\n",
        "    total_sentences = len(sentences)\n",
        "    total_words = len(words)\n",
        "    total_syllables = sum(count_syllables(word) for word in words)\n",
        "\n",
        "    grade = 0.39 * (total_words / total_sentences) + \\\n",
        "            11.8 * (total_syllables / total_words) - 15.59\n",
        "\n",
        "    return round(grade, 2)\n",
        "\n",
        "def bleu_score(reference, candidate):\n",
        "    \"\"\"\n",
        "    BLEU here is computed between ORIGINAL and MODEL OUTPUT.\n",
        "    (You can switch to reference simplification later if you want.)\n",
        "    \"\"\"\n",
        "    reference_tokens = re.findall(r'\\b\\w+\\b', reference.lower())\n",
        "    candidate_tokens = re.findall(r'\\b\\w+\\b', candidate.lower())\n",
        "\n",
        "    reference_list = [reference_tokens]\n",
        "    smoothing = SmoothingFunction().method1\n",
        "\n",
        "    score = sentence_bleu(reference_list, candidate_tokens,\n",
        "                          smoothing_function=smoothing)\n",
        "    return round(score, 4)\n",
        "\n",
        "def sari_score(source, reference, candidate):\n",
        "    \"\"\"\n",
        "    SARI uses:\n",
        "      - source = original\n",
        "      - reference = first human simplification (if present)\n",
        "      - candidate = BART baseline output\n",
        "    \"\"\"\n",
        "    source_tokens    = set(re.findall(r'\\b\\w+\\b', source.lower()))\n",
        "    reference_tokens = set(re.findall(r'\\b\\w+\\b', reference.lower()))\n",
        "    candidate_tokens = set(re.findall(r'\\b\\w+\\b', candidate.lower()))\n",
        "\n",
        "    added   = candidate_tokens - source_tokens\n",
        "    kept    = source_tokens & candidate_tokens\n",
        "    deleted = source_tokens - candidate_tokens\n",
        "\n",
        "    # Add score\n",
        "    if added:\n",
        "        add_precision = len(added & reference_tokens) / len(added)\n",
        "    else:\n",
        "        add_precision = 0.0\n",
        "\n",
        "    # Keep score (F1)\n",
        "    if kept or (source_tokens & reference_tokens):\n",
        "        keep_precision = len(kept & reference_tokens) / len(kept) if kept else 0\n",
        "        keep_recall    = len(kept & reference_tokens) / len(source_tokens & reference_tokens) \\\n",
        "                         if (source_tokens & reference_tokens) else 0\n",
        "        if keep_precision + keep_recall > 0:\n",
        "            keep_f1 = 2 * keep_precision * keep_recall / (keep_precision + keep_recall)\n",
        "        else:\n",
        "            keep_f1 = 0\n",
        "    else:\n",
        "        keep_f1 = 0\n",
        "\n",
        "    # Delete score\n",
        "    reference_deleted = source_tokens - reference_tokens\n",
        "    if deleted:\n",
        "        delete_precision = len(deleted & reference_deleted) / len(deleted)\n",
        "    else:\n",
        "        delete_precision = 0.0\n",
        "\n",
        "    sari = (add_precision + keep_f1 + delete_precision) / 3 * 100\n",
        "    return round(sari, 2)\n",
        "\n",
        "def compression_ratio(original, simplified):\n",
        "    char_ratio = len(simplified) / len(original) if len(original) > 0 else 0\n",
        "\n",
        "    original_words   = len(re.findall(r'\\b\\w+\\b', original))\n",
        "    simplified_words = len(re.findall(r'\\b\\w+\\b', simplified))\n",
        "    word_ratio       = simplified_words / original_words if original_words > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'char_ratio': round(char_ratio, 4),\n",
        "        'word_ratio': round(word_ratio, 4),\n",
        "        'char_reduction_pct': round((1 - char_ratio) * 100, 2),\n",
        "        'word_reduction_pct': round((1 - word_ratio) * 100, 2)\n",
        "    }\n",
        "\n",
        "def average_sentence_length(text):\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    if not sentences:\n",
        "        return {\n",
        "            'avg_sentence_length': 0.0,\n",
        "            'total_sentences': 0\n",
        "        }\n",
        "\n",
        "    total_words = 0\n",
        "    for sentence in sentences:\n",
        "        words = re.findall(r'\\b\\w+\\b', sentence)\n",
        "        total_words += len(words)\n",
        "\n",
        "    avg_length = total_words / len(sentences)\n",
        "\n",
        "    return {\n",
        "        'avg_sentence_length': round(avg_length, 2),\n",
        "        'total_sentences': len(sentences)\n",
        "    }\n",
        "\n",
        "def safe_mean(lst):\n",
        "    return float(np.mean(lst)) if lst else float('nan')\n",
        "\n",
        "# ============================\n",
        "# 4. GENERIC EVAL HELPER\n",
        "# ============================\n",
        "\n",
        "def evaluate_baseline_on_json(\n",
        "    input_json_path: str,\n",
        "    output_json_path: str,\n",
        "    split_name: str,\n",
        "    subset_fraction: float = None\n",
        "):\n",
        "    \"\"\"\n",
        "    - Loads raw test JSON with fields:\n",
        "        \"original\", \"simplifications\" (list of human refs)\n",
        "    - Runs baseline BART on original sentences only\n",
        "    - Computes metrics\n",
        "    - Saves results with bart_baseline + metrics\n",
        "    - Prints corpus-level averages\n",
        "    \"\"\"\n",
        "    with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    n_total = len(data)\n",
        "    if subset_fraction is not None:\n",
        "        n_use = max(1, int(subset_fraction * n_total))\n",
        "        data = data[:n_use]\n",
        "        print(f\"\\nLoaded {n_use} / {n_total} entries from {split_name} (subset_fraction={subset_fraction}).\")\n",
        "    else:\n",
        "        print(f\"\\nLoaded {n_total} entries from {split_name} (full set).\")\n",
        "\n",
        "    print(\"First keys:\", list(data[0].keys()))\n",
        "\n",
        "    metric_agg = {\n",
        "        \"fkg_original\": [],\n",
        "        \"fkg_bart\": [],\n",
        "        \"bleu_original_bart\": [],\n",
        "        \"sari_bart_vs_ref0\": [],\n",
        "        \"word_ratio\": [],\n",
        "        \"char_ratio\": [],\n",
        "        \"asl_original\": [],\n",
        "        \"asl_bart\": [],\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, entry in enumerate(data):\n",
        "        original = entry[\"original\"]\n",
        "        refs = entry.get(\"simplifications\", [])\n",
        "\n",
        "        # 1) Model generates its own simplification from ORIGINAL only\n",
        "        bart_text = simplify_baseline(original)\n",
        "\n",
        "        # 2) Metrics\n",
        "        fkg_orig = flesch_kincaid_grade(original)\n",
        "        fkg_bart = flesch_kincaid_grade(bart_text)\n",
        "\n",
        "        bleu_bart = bleu_score(original, bart_text)\n",
        "\n",
        "        comp      = compression_ratio(original, bart_text)\n",
        "        asl_orig  = average_sentence_length(original)\n",
        "        asl_bart  = average_sentence_length(bart_text)\n",
        "\n",
        "        if refs:\n",
        "            sari_bart = sari_score(\n",
        "                source=original,\n",
        "                reference=refs[0],   # first human simplification\n",
        "                candidate=bart_text\n",
        "            )\n",
        "        else:\n",
        "            sari_bart = None\n",
        "\n",
        "        metrics = {\n",
        "            \"fkg_original\": fkg_orig,\n",
        "            \"fkg_bart\": fkg_bart,\n",
        "            \"bleu_original_bart\": bleu_bart,\n",
        "            \"compression\": comp,\n",
        "            \"asl_original\": asl_orig,\n",
        "            \"asl_bart\": asl_bart,\n",
        "            \"sari_bart_vs_first_ref\": sari_bart,\n",
        "        }\n",
        "\n",
        "        result_entry = {\n",
        "            \"index\": i,\n",
        "            \"original\": original,\n",
        "            \"simplifications\": refs,\n",
        "            \"bart_baseline\": bart_text,\n",
        "            \"metrics\": metrics,\n",
        "        }\n",
        "        results.append(result_entry)\n",
        "\n",
        "        # Aggregate\n",
        "        metric_agg[\"fkg_original\"].append(fkg_orig)\n",
        "        metric_agg[\"fkg_bart\"].append(fkg_bart)\n",
        "        metric_agg[\"bleu_original_bart\"].append(bleu_bart)\n",
        "        metric_agg[\"word_ratio\"].append(comp[\"word_ratio\"])\n",
        "        metric_agg[\"char_ratio\"].append(comp[\"char_ratio\"])\n",
        "        metric_agg[\"asl_original\"].append(asl_orig[\"avg_sentence_length\"])\n",
        "        metric_agg[\"asl_bart\"].append(asl_bart[\"avg_sentence_length\"])\n",
        "        if sari_bart is not None:\n",
        "            metric_agg[\"sari_bart_vs_ref0\"].append(sari_bart)\n",
        "\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"  Processed {i+1}/{len(data)} examples in {split_name}...\")\n",
        "\n",
        "    # Save JSON with metrics\n",
        "    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
        "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"\\nSaved {split_name} baseline outputs + metrics to:\\n{output_json_path}\")\n",
        "\n",
        "    # Corpus-level stats\n",
        "    print(f\"\\n=== CORPUS-LEVEL AVERAGES (BART BASELINE â€“ {split_name}) ===\")\n",
        "    print(\"Flesch-Kincaid (original):\", safe_mean(metric_agg[\"fkg_original\"]))\n",
        "    print(\"Flesch-Kincaid (BART):    \", safe_mean(metric_agg[\"fkg_bart\"]))\n",
        "    print(\"BLEU (original vs BART):  \", safe_mean(metric_agg[\"bleu_original_bart\"]))\n",
        "    print(\"Word ratio (BART/orig):   \", safe_mean(metric_agg[\"word_ratio\"]))\n",
        "    print(\"Char ratio (BART/orig):   \", safe_mean(metric_agg[\"char_ratio\"]))\n",
        "    print(\"ASL original (words/s):   \", safe_mean(metric_agg[\"asl_original\"]))\n",
        "    print(\"ASL BART (words/s):       \", safe_mean(metric_agg[\"asl_bart\"]))\n",
        "    print(\"SARI (if refs present):   \", safe_mean(metric_agg[\"sari_bart_vs_ref0\"]))\n",
        "\n",
        "\n",
        "# ============================\n",
        "# 5. RUN EVALS:\n",
        "#    - FULL asset test\n",
        "#    - 10% synthetic test\n",
        "# ============================\n",
        "\n",
        "ASSET_TEST_JSON   = \"/content/drive/My Drive/AML_Final_Project/Data/asset_test.json\"\n",
        "SYNTH_TEST_JSON   = \"/content/drive/My Drive/AML_Final_Project/Data/synthetic_test.json\"\n",
        "\n",
        "ASSET_OUT_JSON    = \"/content/drive/My Drive/colab_data/baseline_notrain_asset_test_with_metrics.json\"\n",
        "SYNTH_10P_OUT_JSON = \"/content/drive/My Drive/colab_data/baseline_notrain_synthetic_test_10p_with_metrics.json\"\n",
        "\n",
        "# 1) FULL asset test\n",
        "evaluate_baseline_on_json(\n",
        "    input_json_path=ASSET_TEST_JSON,\n",
        "    output_json_path=ASSET_OUT_JSON,\n",
        "    split_name=\"ASSET_TEST_FULL\",\n",
        "    subset_fraction=None,        # full\n",
        ")\n",
        "\n",
        "# 2) 10% synthetic test\n",
        "evaluate_baseline_on_json(\n",
        "    input_json_path=SYNTH_TEST_JSON,\n",
        "    output_json_path=SYNTH_10P_OUT_JSON,\n",
        "    split_name=\"SYNTH_TEST_10P\",\n",
        "    subset_fraction=0.10,        # 10%\n",
        ")\n",
        "\n",
        "print(\"\\n=== DONE: baseline (no-train) BART evaluated on full asset test + 10% synthetic test ===\")\n"
      ],
      "metadata": {
        "id": "m4_LzpTnsRuN"
      },
      "id": "m4_LzpTnsRuN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}