{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXsIyWeIOC_a",
        "outputId": "ca8df41e-bec7-4bbe-825e-8e18773272bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy4Xryf7ODxm",
        "outputId": "5f4390fd-1008-4ca5-9164-664900fb628c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers version: 4.57.3\n",
            "Datasets version: 4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \"transformers>=4.45.0\" \"datasets>=2.20.0\" accelerate sentencepiece\n",
        "\n",
        "import transformers, datasets\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "print(\"Datasets version:\", datasets.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK1FW5zcW-AT"
      },
      "source": [
        "# 0. Import & Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "07-NCAlLR6IW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6c89c16-6445-4bce-d40d-743a603de23e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from typing import Dict, Any\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "# Metrics\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import cmudict\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('corpora/cmudict')\n",
        "except LookupError:\n",
        "    nltk.download('cmudict')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "VR4ML6aDSy3S"
      },
      "outputs": [],
      "source": [
        "# Paths to JSON files\n",
        "TRAIN_JSON_1 = \"/content/drive/MyDrive/AML_Final_Project/Data/synthetic_train.json\"\n",
        "TRAIN_JSON_2 = \"/content/drive/MyDrive/AML_Final_Project/Data/asset_train.json\"\n",
        "\n",
        "# TEST_JSON  = \"/content/drive/MyDrive/AML_Final_Project/Data/synthetic_test.json\"\n",
        "TEST_JSON  = \"/content/drive/MyDrive/AML_Final_Project/Data/asset_test.json\"\n",
        "\n",
        "MODEL_NAME = \"facebook/bart-base\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/AML_Final_Project/bart-config\"\n",
        "\n",
        "MAX_SOURCE_LENGTH = 128   # input: prompt + original, 80~100 words, 1024 limit\n",
        "MAX_TARGET_LENGTH = 64    # output: simplified, 40~50 words\n",
        "\n",
        "# Training hyperparameters\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 3e-5\n",
        "GRAD_CLIP = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vCqOpfHKHs2j"
      },
      "outputs": [],
      "source": [
        "TASK_PREFIX = (\n",
        "    \"Explain this in simple, plain language for a general audience. \"\n",
        "    \"Use short sentences and everyday words, but keep all important information.\\n\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bELuTdXXCNV"
      },
      "source": [
        "# 1. GPU Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hu98OumxR_id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38f094d-0421-443b-dcb1-156db6213000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y15nWXz_XEqo"
      },
      "source": [
        "# 2. Load Json Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "oRntN-OfS2GB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "450dd83927e845e596df8e1ea5b53c52",
            "ac16aaa7e12a48809d3f6bc1bca6abf1",
            "e4d101a3d7f74b17a53bd1e923e9f04b",
            "6593abb7ac814261ad70f02011abf559",
            "432c415d5cfe4f4fa4f344b89bc56627",
            "0438d12734374b2d90a4d1bd1a7003b5",
            "2ef5be128e12434593b8ef6b7a622b93",
            "b990fbecbf724542b436d32c9b516065",
            "b60f5a6105164e3ca2f88658b30e26e0",
            "e82edf10f1764a6687aa2571e33e5fa4",
            "3a0288f659c64de7a8130b22fd223e6c",
            "a5264adf37eb4ed5a80cc3075b1e627a",
            "81996cfdaa3f4afd9ba0096f24a107f1",
            "9b2444170d084d02a01454e253c4c797",
            "783a968cc5c84a9598b48f61c2ccae81",
            "c24afcd6e9ef49738bfbd406bf5fcab9",
            "0886253f001d4d8d84fc2d6e5fc89431",
            "e0dc466e3db245a99efb3e008150b371",
            "8be3e63268024e07b0f616d044576780",
            "1783f13ca3454304a4ce22986d2bf7bc",
            "59c49f15d4f4477aaacf999680b758d6",
            "ca03ff46db57419ba15212d110770d89"
          ]
        },
        "outputId": "b2cb126c-8190-46f8-d26f-5766d0e79f7a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "450dd83927e845e596df8e1ea5b53c52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating eval split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5264adf37eb4ed5a80cc3075b1e627a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw train entries: 27330\n",
            "Raw eval entries: 359\n"
          ]
        }
      ],
      "source": [
        "raw_datasets = load_dataset(\n",
        "    \"json\",\n",
        "    data_files={\n",
        "        \"train\": [TRAIN_JSON_1, TRAIN_JSON_2],\n",
        "        \"eval\": TEST_JSON,\n",
        "    },\n",
        ")\n",
        "\n",
        "train_dataset = raw_datasets[\"train\"]\n",
        "eval_dataset = raw_datasets[\"eval\"]\n",
        "\n",
        "print(\"Raw train entries:\", len(train_dataset))\n",
        "print(\"Raw eval entries:\", len(eval_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--ebPlyNXPAm"
      },
      "source": [
        "# 3. Flatten Multiple Simplifications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215,
          "referenced_widgets": [
            "4b4da70864cd4b4d9738e19743854939",
            "e72ed18b9df44f5c93d8389b1444abca",
            "c10c4a288551482facb6ddda1ac3e9c4",
            "a48553d0efe14841880d3a3ea9d27291",
            "83c574ea53934a44a2bb3501b7b936d6",
            "156b1b7b529442e7880d06851ce35ee1",
            "776dc8095d194ca1b618be7ef6b4332e",
            "c9ed6e0e5d0d4cd7b2b0f955d4c617c0",
            "fc4bdb8c7e7c4cf79335b73ac1cdbd43",
            "15fb1a525ada412da8e9792322eead93",
            "a144abf25466463e8fd82fa559552db8",
            "5380783d8eaf4b889177d98286772a5c",
            "8df0a097219e41f891e36cb52c8ac43e",
            "13fafdc0ff1e48d394a56824d77a4e19",
            "c97101b2c8b843e6b179b217caa06ea2",
            "195c75be62ec4f1ca2cb92447fbeca3e",
            "2e8df2e8d6a045e69741b93e8cfb6ed0",
            "f3778ae8e6dc4a12a95712b2b4cc984e",
            "14d3536f097348279df2404a109eb3df",
            "48167012f1e6460ea71e3a07586dce17",
            "ba7625bb3d6a410bb1afa883b6cf046d",
            "ba3d2b6a6cac4d9fafcdb2509f8a87b2"
          ]
        },
        "id": "jDW8PdQZS6rP",
        "outputId": "db4243fd-29f3-49b6-fc22-30a72076a891"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/27330 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b4da70864cd4b4d9738e19743854939"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/359 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5380783d8eaf4b889177d98286772a5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset train examples: 45330\n",
            "Subset eval examples: 3590\n"
          ]
        }
      ],
      "source": [
        "# Turn:\n",
        "#   original = \"...\"\n",
        "#   simplifications = [\"a\", \"b\", \"c\"]\n",
        "# into:\n",
        "#   (original, \"a\"), (original, \"b\"), (original, \"c\")\n",
        "\n",
        "def explode_simplifications(examples: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    new_originals = []\n",
        "    new_simplified = []\n",
        "\n",
        "    for orig, sims in zip(examples[\"original\"], examples[\"simplifications\"]):\n",
        "        for s in sims:\n",
        "            new_originals.append(orig)\n",
        "            new_simplified.append(s)\n",
        "\n",
        "    return {\n",
        "        \"original\": new_originals,\n",
        "        \"simplified\": new_simplified,\n",
        "    }\n",
        "\n",
        "train_flat = train_dataset.map(\n",
        "    explode_simplifications,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        ")\n",
        "\n",
        "eval_flat = eval_dataset.map(\n",
        "    explode_simplifications,\n",
        "    batched=True,\n",
        "    remove_columns=eval_dataset.column_names,\n",
        ")\n",
        "\n",
        "print(\"Subset train examples:\", len(train_flat))\n",
        "print(\"Subset eval examples:\", len(eval_flat))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB_cTpnTXkrg"
      },
      "source": [
        "# 4. Tokenizer & Model (BART)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "561c8e7b11fc4e3fb59b26b687280927",
            "d74538697bd94a07b0a8996d631302b1",
            "5ba5556aa607458a995c0009b5ae50e5",
            "45d9183ab2174f26937d8671fbc2ded4",
            "83d74b734be24e37b27a3d8a7e21e879",
            "6c4f223334b04c26a1dacf56b8803b54",
            "45774afcb1c74cf7b26b109e78697dab",
            "0fd1c42463334c7ca9171a73d2975ab2",
            "c9fc7d44e4a0499faf8663d21b03c873",
            "5cf8b95e78854e8fab94a12ce8ecf90d",
            "f3c30e52f5d141f6bb05978008390446",
            "45f9d2e00d2545bead65a28e3032af16",
            "a2deebeebce34e1c91b9142a70d1c1f9",
            "bb7691b50dce4d0fb4f84720835d5662",
            "02b0ad8c0ad840b6b8f4c4e175d7fe87",
            "1d2615598fc74cd59fb8a0ebe11e1c86",
            "22ef815781fe4cfca31b793fccd90e3c",
            "32168891293e4f00a36310d33ae3c250",
            "bd9b718afb8c4a43b82c56db8d35bb1f",
            "c12beff5b5444f928b0590a7973f1c45",
            "32a89941a03245bbab8f5e214348c92a",
            "f55390ba46024f23985d3b8f442e4c42",
            "8cacd30b46524edda3c4d205ccbd61ce",
            "41304c1393f540e2864045b0909a2c6f",
            "b38da24350b64b05915b028a42fbb294",
            "89b88aa9a0714d2aa70f6c6dab12154b",
            "2295e8e5410e435a8afe4d04df39d2ee",
            "a7d776a6b43140769105bd290ae6192c",
            "92bb18e7358c4c64ab0c7e056ae60399",
            "755bf6789d3443b59914642bc855d8d9",
            "1ce0a5b8ede8473892a7d46c38f962cc",
            "05e6d1e697c942ab8372bc0ab729ae8b",
            "4a126ea764ef47488b04ef5d1eb3bfd6",
            "8c03e7b0d8e74f1d8464431c3589755d",
            "63cb3b987fd14afa88f9292db4d76a9b",
            "7f97b54bb3a1480aa628e4987c9eafdd",
            "f144a52616d94963b8a466645489cefb",
            "fd99102193644683bd908df2143df171",
            "1763be799ca945fea700295147ae200e",
            "fd6a86b2fbf94ba293f24e7490ec8576",
            "0dd237a1015546359f3d0c26f02f242e",
            "c2bdbee0f6b84823bb55bd584b6787f4",
            "34d34d9967c04262b420986da1b727f3",
            "d199ecda1e8b436ebd6556a9c595b4d9",
            "edd5d0595e814bc7b7a4f1e863dfe5b2",
            "e7617c84b2924790a90bf61ee2492e2a",
            "83934dcc1ea94dde94c1148a8b6fd3f6",
            "8fae96cec7874cfcaec9c9d19601a0b1",
            "8e1979fb86b940a8aaa01a9475903010",
            "8a76618a05f9445e80b8e36c02e8b4a3",
            "657879fb90674bc1a9cad779cbbbbd55",
            "19e498f364df49ae955e16805ed4c4ea",
            "5e95eb6a18cb4a32bbbbf1bdcaeeac9f",
            "83bd742e0596401ca7e4d324d45b4ad4",
            "cee2ebcfadfe4476aefa869cc8f8c986"
          ]
        },
        "collapsed": true,
        "id": "FCXP8rNaS_1u",
        "outputId": "6c379f1c-3732-4321-d12b-ac491f9092da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "561c8e7b11fc4e3fb59b26b687280927"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45f9d2e00d2545bead65a28e3032af16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cacd30b46524edda3c4d205ccbd61ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c03e7b0d8e74f1d8464431c3589755d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edd5d0595e814bc7b7a4f1e863dfe5b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "config.use_cache = False\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, config=config)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEleY0sJXvmN"
      },
      "source": [
        "# 5. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135,
          "referenced_widgets": [
            "80732af174334799a7e2b40ca25401cb",
            "e76268dbfe8643748fa0cfb33391d986",
            "49dada5aa27e4ad58f57edeb86a43902",
            "7e229c12f3d24b46abffcee9b9d4c292",
            "950c8f79148b48078c37f5de89e7b970",
            "121760bc24e2418aa07718fc660ac5a1",
            "2972c35bc0404b04b001209d1ea69c1f",
            "b9fc4c9ffaa14b488b76daa7fa4e17c1",
            "b362e80e360548c299019537b2957720",
            "71b61fa8c75d438ca49d2078d04f9faa",
            "e8f54aebf3154ff9bfe791b2cc768596",
            "2d0bb78af069421fb7493be0248795a3",
            "391b8aebd6a244419eeae538ac375cd5",
            "4675124da81f49b186a7f32088041732",
            "5309d5cd54a5445d94c1b11afe304644",
            "3923a7b579a64d938fc19ddc00930ffc",
            "3569570d23bd41e68b8f1ef5b7f5c6ed",
            "afb3c2a9a07b4e25be10a6dd377d8b14",
            "8d7af0057c664280bb1852dd9a5830fd",
            "548f70a96e9645339f8274abf2f816fc",
            "5e205481557d44b091fd42147d7c52ff",
            "e5f4cbec8dbf45748b7e90c83ab2ec43"
          ]
        },
        "id": "wF6mPtEvYNj3",
        "outputId": "5eb04648-973d-4fcd-9201-7c21536c5673"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/45330 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80732af174334799a7e2b40ca25401cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4470 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d0bb78af069421fb7493be0248795a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized train sample keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
          ]
        }
      ],
      "source": [
        "def preprocess_function(examples: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    # Build source strings with the task prefix + original text\n",
        "    sources = [\n",
        "        text.strip() for text in examples[\"original\"]\n",
        "    ]\n",
        "    targets = [text.strip() for text in examples[\"simplified\"]]\n",
        "\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(\n",
        "        sources,\n",
        "        truncation=True,\n",
        "        padding=True, # \"max_length\" for fixed shapes\n",
        "        # max_length=MAX_SOURCE_LENGTH,\n",
        "    )\n",
        "\n",
        "    labels = tokenizer(\n",
        "        text_target=targets,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        # max_length=MAX_TARGET_LENGTH,\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_train = train_flat.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_flat.column_names,\n",
        ")\n",
        "\n",
        "tokenized_eval = eval_flat.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=eval_flat.column_names,\n",
        ")\n",
        "\n",
        "print(\"Tokenized train sample keys:\", tokenized_train[0].keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T39FruwYOxO"
      },
      "source": [
        "# 6. Dataloaders & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UW3s3PIJYXXO"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    # max_length=MAX_SOURCE_LENGTH,\n",
        "    label_pad_token_id=-100,\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_train,\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_eval,\n",
        "    shuffle=False,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nZR-fNhpnAD"
      },
      "source": [
        "# 7. Training (seq2seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2wMKojLYXuQi"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    save_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    predict_with_generate=False,\n",
        "    report_to=\"none\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NyQ2BnNZXwKu"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESAWF9X6X8qs",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV8TOQ9hX_OP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logs = trainer.state.log_history\n",
        "\n",
        "train_loss = [x[\"loss\"] for x in logs if \"loss\" in x and \"epoch\" in x]\n",
        "train_ep   = [x[\"epoch\"] for x in logs if \"loss\" in x and \"epoch\" in x]\n",
        "\n",
        "eval_loss  = [x[\"eval_loss\"] for x in logs if \"eval_loss\" in x]\n",
        "eval_ep    = [x[\"epoch\"] for x in logs if \"eval_loss\" in x]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(train_ep, train_loss, marker=\"o\", label=\"Train loss\")\n",
        "plt.plot(eval_ep,  eval_loss,  marker=\"o\", label=\"Eval loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train vs Eval Loss\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Training complete. Model saved to:\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "UwOi3YImX8UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrZ1-Nb_ZGUQ"
      },
      "source": [
        "# 8. Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seq2seq\n",
        "FINAL_CKPT = \"/content/drive/MyDrive/AML_Final_Project/bart-config/checkpoint-final\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(FINAL_CKPT)\n",
        "inference_model = AutoModelForSeq2SeqLM.from_pretrained(FINAL_CKPT).to(device)\n",
        "\n",
        "inference_model.eval()\n",
        "inference_model.config.use_cache = True\n"
      ],
      "metadata": {
        "id": "iOSOAavdzsTv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7Y672SFTGO6y"
      },
      "outputs": [],
      "source": [
        "gen_config = GenerationConfig.from_model_config(inference_model.config)\n",
        "\n",
        "gen_config.use_cache = False\n",
        "gen_config.num_beams = 4\n",
        "gen_config.max_new_tokens = 64\n",
        "gen_config.early_stopping = True\n",
        "gen_config.forced_bos_token_id = 0\n",
        "gen_config.no_repeat_ngram_size = 3\n",
        "gen_config.length_penalty = 1.1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TPEy2A61Us3F"
      },
      "outputs": [],
      "source": [
        "def simplify_trained(text: str, max_new_tokens: int = 64, num_beams: int = 4) -> str:\n",
        "    model_input = text.strip()\n",
        "\n",
        "    enc = tokenizer(\n",
        "        model_input,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_SOURCE_LENGTH,\n",
        "    )\n",
        "    enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = inference_model.generate(\n",
        "            **enc,\n",
        "            generation_config=gen_config\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True) # seq2seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoLlXBJhZ3Hk",
        "outputId": "52c4327e-3438-48ae-a943-191c980cf9a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ORIGINAL ===\n",
            "\n",
            "Jeddah is the principal gateway to Mecca, Islam's holiest city, which able-bodied Muslims are required to visit at least once in their lifetime.\n",
            "\n",
            "=== SIMPLIFIED (MODEL OUTPUT) ===\n",
            "\n",
            "Jeddah is the main gateway to Mecca, Islam's holiest city, which Muslims need to visit at least once in their lifetime.\n"
          ]
        }
      ],
      "source": [
        "test_text = (\n",
        "    # \"Adjacent counties are Marin (to the south), Mendocino (to the north), \"\n",
        "    # \"Lake (northeast), Napa (to the east), and Solano and Contra Costa (to the southeast).\"\n",
        "    \"Jeddah is the principal gateway to Mecca, Islam's holiest city, \"\n",
        "    \"which able-bodied Muslims are required to visit at least once in their lifetime.\"\n",
        "    )\n",
        "\n",
        "print(\"\\n=== ORIGINAL ===\\n\")\n",
        "print(test_text)\n",
        "\n",
        "print(\"\\n=== SIMPLIFIED (MODEL OUTPUT) ===\\n\")\n",
        "print(simplify_trained(test_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co-1XgNTxJ-R"
      },
      "source": [
        "# 9. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-myqIZOD6h-D"
      },
      "source": [
        "## 9-1. Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "d5LpGWdZ5PF_"
      },
      "outputs": [],
      "source": [
        "# Initialize CMU Pronouncing Dictionary for syllable counting\n",
        "d = cmudict.dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bAOfBZcC6kW6"
      },
      "outputs": [],
      "source": [
        "def count_syllables(word):\n",
        "    \"\"\"Count syllables in a word using CMU Pronouncing Dictionary.\"\"\"\n",
        "    word = word.lower()\n",
        "    if word in d:\n",
        "        return max([len([y for y in x if y[-1].isdigit()]) for x in d[word]])\n",
        "    else:\n",
        "        # Fallback: estimate syllables by counting vowel groups\n",
        "        word = word.lower()\n",
        "        count = 0\n",
        "        vowels = 'aeiouy'\n",
        "        previous_was_vowel = False\n",
        "        for char in word:\n",
        "            is_vowel = char in vowels\n",
        "            if is_vowel and not previous_was_vowel:\n",
        "                count += 1\n",
        "            previous_was_vowel = is_vowel\n",
        "        # Adjust for silent 'e'\n",
        "        if word.endswith('e'):\n",
        "            count -= 1\n",
        "        if count == 0:\n",
        "            count = 1\n",
        "        return count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8ZdLr34Z6l5H"
      },
      "outputs": [],
      "source": [
        "def flesch_kincaid_grade(text):\n",
        "    \"\"\"\n",
        "    Calculate Flesch-Kincaid Grade Level.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to analyze\n",
        "\n",
        "    Returns:\n",
        "        float: The grade level score\n",
        "    \"\"\"\n",
        "    # Split into sentences\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    # Split into words\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "    if not sentences or not words:\n",
        "        return 0.0\n",
        "\n",
        "    total_sentences = len(sentences)\n",
        "    total_words = len(words)\n",
        "    total_syllables = sum(count_syllables(word) for word in words)\n",
        "\n",
        "    # Flesch-Kincaid Grade Level formula\n",
        "    # https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests\n",
        "    grade = 0.39 * (total_words / total_sentences) + 11.8 * (total_syllables / total_words) - 15.59\n",
        "\n",
        "    return round(grade, 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qI0xCO-N6pQx"
      },
      "outputs": [],
      "source": [
        "def bleu_score(reference, candidate):\n",
        "    \"\"\"\n",
        "    Calculate BLEU score between reference and candidate text.\n",
        "\n",
        "    BLEU measures how similar the candidate text is to the reference text.\n",
        "    Score ranges from 0 to 1, where 1 indicates perfect match.\n",
        "\n",
        "    Args:\n",
        "        reference (str): The reference (original) text\n",
        "        candidate (str): The candidate (simplified) text\n",
        "\n",
        "    Returns:\n",
        "        float: BLEU score between 0 and 1\n",
        "    \"\"\"\n",
        "    # Tokenize texts into words\n",
        "    reference_tokens = re.findall(r'\\b\\w+\\b', reference.lower())\n",
        "    candidate_tokens = re.findall(r'\\b\\w+\\b', candidate.lower())\n",
        "\n",
        "    # BLEU expects reference as list of lists\n",
        "    reference_list = [reference_tokens]\n",
        "\n",
        "    # Use smoothing to handle cases with no n-gram matches\n",
        "    smoothing = SmoothingFunction().method1\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    score = sentence_bleu(reference_list, candidate_tokens, smoothing_function=smoothing)\n",
        "\n",
        "    return round(score, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TY4wXHrU6rfc"
      },
      "outputs": [],
      "source": [
        "def sari_score(source, reference, candidate):\n",
        "    \"\"\"\n",
        "    Calculate SARI score for text simplification.\n",
        "\n",
        "    SARI measures the quality of simplification by evaluating:\n",
        "    - Added words (should be simple/appropriate)\n",
        "    - Deleted words (should remove complex content)\n",
        "    - Kept words (should retain important information)\n",
        "\n",
        "    Args:\n",
        "        source (str): The original source text\n",
        "        reference (str): The reference simplified text\n",
        "        candidate (str): The candidate simplified text\n",
        "\n",
        "    Returns:\n",
        "        float: SARI score (0-100 scale)\n",
        "    \"\"\"\n",
        "    # Tokenize texts\n",
        "    source_tokens = set(re.findall(r'\\b\\w+\\b', source.lower()))\n",
        "    reference_tokens = set(re.findall(r'\\b\\w+\\b', reference.lower()))\n",
        "    candidate_tokens = set(re.findall(r'\\b\\w+\\b', candidate.lower()))\n",
        "\n",
        "    # Calculate add, keep, and delete operations\n",
        "    # Add: words in candidate but not in source\n",
        "    added = candidate_tokens - source_tokens\n",
        "    # Keep: words in both source and candidate\n",
        "    kept = source_tokens & candidate_tokens\n",
        "    # Delete: words in source but not in candidate\n",
        "    deleted = source_tokens - candidate_tokens\n",
        "\n",
        "    # Calculate precision and recall for each operation\n",
        "    # Add score: precision of added words (how many added words are in reference)\n",
        "    if added:\n",
        "        add_precision = len(added & reference_tokens) / len(added)\n",
        "    else:\n",
        "        add_precision = 0.0\n",
        "\n",
        "    # Keep score: F1 of kept words\n",
        "    if kept or (source_tokens & reference_tokens):\n",
        "        keep_precision = len(kept & reference_tokens) / len(kept) if kept else 0\n",
        "        keep_recall = len(kept & reference_tokens) / len(source_tokens & reference_tokens) if (source_tokens & reference_tokens) else 0\n",
        "        if keep_precision + keep_recall > 0:\n",
        "            keep_f1 = 2 * keep_precision * keep_recall / (keep_precision + keep_recall)\n",
        "        else:\n",
        "            keep_f1 = 0\n",
        "    else:\n",
        "        keep_f1 = 0\n",
        "\n",
        "    # Delete score: precision of deleted words (how many deleted words are also deleted in reference)\n",
        "    reference_deleted = source_tokens - reference_tokens\n",
        "    if deleted:\n",
        "        delete_precision = len(deleted & reference_deleted) / len(deleted)\n",
        "    else:\n",
        "        delete_precision = 0.0\n",
        "\n",
        "    # SARI is the average of the three scores (scaled to 0-100)\n",
        "    sari = (add_precision + keep_f1 + delete_precision) / 3 * 100\n",
        "\n",
        "    return round(sari, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-iiuAWXc6uuC"
      },
      "outputs": [],
      "source": [
        "def compression_ratio(original, simplified):\n",
        "    \"\"\"\n",
        "    Calculate compression ratio between original and simplified text.\n",
        "\n",
        "    Measures how much shorter the simplified text is compared to the original.\n",
        "    Typical good simplifications: 0.6-0.8 (20-40% shorter)\n",
        "\n",
        "    Args:\n",
        "        original (str): The original text\n",
        "        simplified (str): The simplified text\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with character-based and word-based compression ratios\n",
        "    \"\"\"\n",
        "    # Character-based compression\n",
        "    char_ratio = len(simplified) / len(original) if len(original) > 0 else 0\n",
        "\n",
        "    # Word-based compression\n",
        "    original_words = len(re.findall(r'\\b\\w+\\b', original))\n",
        "    simplified_words = len(re.findall(r'\\b\\w+\\b', simplified))\n",
        "    word_ratio = simplified_words / original_words if original_words > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'char_ratio': round(char_ratio, 4),\n",
        "        'word_ratio': round(word_ratio, 4),\n",
        "        'char_reduction_pct': round((1 - char_ratio) * 100, 2),\n",
        "        'word_reduction_pct': round((1 - word_ratio) * 100, 2)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hzeN3nic6w8R"
      },
      "outputs": [],
      "source": [
        "def average_sentence_length(text):\n",
        "    \"\"\"\n",
        "    Calculate average sentence length in words.\n",
        "\n",
        "    Simpler text typically has shorter sentences.\n",
        "    General guidelines:\n",
        "    - <15 words: Very easy\n",
        "    - 15-20 words: Easy\n",
        "    - 20-25 words: Moderate\n",
        "    - >25 words: Difficult\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to analyze\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with average sentence length and total sentences\n",
        "    \"\"\"\n",
        "    # Split into sentences\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    if not sentences:\n",
        "        return {\n",
        "            'avg_sentence_length': 0.0,\n",
        "            'total_sentences': 0\n",
        "        }\n",
        "\n",
        "    # Count words in each sentence\n",
        "    total_words = 0\n",
        "    for sentence in sentences:\n",
        "        words = re.findall(r'\\b\\w+\\b', sentence)\n",
        "        total_words += len(words)\n",
        "\n",
        "    avg_length = total_words / len(sentences)\n",
        "\n",
        "    return {\n",
        "        'avg_sentence_length': round(avg_length, 2),\n",
        "        'total_sentences': len(sentences)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iMZSCVCw7WXx"
      },
      "outputs": [],
      "source": [
        "def print_results(results):\n",
        "    \"\"\"Pretty print the aggregated results.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"DATASET METRICS (n={results['dataset_size']} pairs)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\n\ud83d\udcca READABILITY (Flesch-Kincaid Grade Level)\")\n",
        "    print(f\"  Original:    {results['flesch_kincaid']['original']['mean']:.2f} \u00b1 {results['flesch_kincaid']['original']['std']:.2f}\")\n",
        "    print(f\"  Simplified:  {results['flesch_kincaid']['simplified']['mean']:.2f} \u00b1 {results['flesch_kincaid']['simplified']['std']:.2f}\")\n",
        "    print(f\"  Improvement: {results['flesch_kincaid']['improvement']:.2f} grade levels\")\n",
        "\n",
        "    print(\"\\n\ud83d\udcdd SEMANTIC SIMILARITY (BLEU Score)\")\n",
        "    print(f\"  Mean:   {results['bleu']['mean']:.4f} \u00b1 {results['bleu']['std']:.4f}\")\n",
        "    print(f\"  Median: {results['bleu']['median']:.4f}\")\n",
        "\n",
        "    print(\"\\n\ud83d\udccf COMPRESSION\")\n",
        "    print(f\"  Word Ratio:  {results['compression']['word_ratio']['mean']:.4f} ({results['compression']['avg_word_reduction_pct']:.1f}% reduction)\")\n",
        "    print(f\"  Char Ratio:  {results['compression']['char_ratio']['mean']:.4f}\")\n",
        "\n",
        "    print(\"\\n\ud83d\udcd0 SENTENCE LENGTH (words/sentence)\")\n",
        "    print(f\"  Original:    {results['avg_sentence_length']['original']['mean']:.2f} \u00b1 {results['avg_sentence_length']['original']['std']:.2f}\")\n",
        "    print(f\"  Simplified:  {results['avg_sentence_length']['simplified']['mean']:.2f} \u00b1 {results['avg_sentence_length']['simplified']['std']:.2f}\")\n",
        "    print(f\"  Reduction:   {results['avg_sentence_length']['reduction']:.2f} words\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhwYlobp67Km"
      },
      "source": [
        "## 9-2. Calculate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "z6ygcGmU5Qcg"
      },
      "outputs": [],
      "source": [
        "def generate_predictions(data, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    for item in tqdm(data, desc=\"Generating predictions\"):\n",
        "        src = item[\"original\"].strip()\n",
        "\n",
        "        enc = tokenizer(\n",
        "            src,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=MAX_SOURCE_LENGTH,\n",
        "        )\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out_ids = model.generate(\n",
        "                **enc,\n",
        "                generation_config=gen_config\n",
        "            )\n",
        "\n",
        "        pred = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "        preds.append(pred)\n",
        "\n",
        "    return preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_azj4h4sxQWK"
      },
      "outputs": [],
      "source": [
        "def calculate_dataset_metrics(data, preds):\n",
        "    fk_original_scores = []\n",
        "    fk_pred_scores = []\n",
        "\n",
        "    bleu_scores = []\n",
        "    sari_scores = []\n",
        "\n",
        "    compression_word_ratios = []\n",
        "    compression_char_ratios = []\n",
        "\n",
        "    asl_original_scores = []\n",
        "    asl_pred_scores = []\n",
        "\n",
        "    for item, pred in tqdm(list(zip(data, preds)), desc=\"Scoring metrics\"):\n",
        "        original = item[\"original\"]\n",
        "        references = item.get(\"simplifications\", [item.get(\"simplified\", \"\")])\n",
        "\n",
        "        # FKGL\n",
        "        fk_original_scores.append(flesch_kincaid_grade(original))\n",
        "        fk_pred_scores.append(flesch_kincaid_grade(pred))\n",
        "\n",
        "        # BLEU: reference(s) vs candidate(pred)\n",
        "        bleu_per_refs = [bleu_score(ref, pred) for ref in references if ref and ref.strip()]\n",
        "        bleu_scores.append(max(bleu_per_refs) if bleu_per_refs else 0.0)\n",
        "\n",
        "        # SARI approximation: average across refs\n",
        "        sari_per_refs = [sari_score(original, ref, pred) for ref in references if ref and ref.strip()]\n",
        "        sari_scores.append(float(np.mean(sari_per_refs)) if sari_per_refs else 0.0)\n",
        "\n",
        "        # Compression\n",
        "        comp = compression_ratio(original, pred)\n",
        "        compression_word_ratios.append(comp[\"word_ratio\"])\n",
        "        compression_char_ratios.append(comp[\"char_ratio\"])\n",
        "\n",
        "        # Avg sentence length\n",
        "        asl_orig = average_sentence_length(original)[\"avg_sentence_length\"]\n",
        "        asl_p = average_sentence_length(pred)[\"avg_sentence_length\"]\n",
        "        asl_original_scores.append(asl_orig)\n",
        "        asl_pred_scores.append(asl_p)\n",
        "\n",
        "    results = {\n",
        "        \"dataset_size\": len(data),\n",
        "\n",
        "        \"flesch_kincaid\": {\n",
        "            \"original\": {\n",
        "                \"mean\": round(float(np.mean(fk_original_scores)), 2),\n",
        "                \"std\": round(float(np.std(fk_original_scores)), 2),\n",
        "                \"median\": round(float(np.median(fk_original_scores)), 2),\n",
        "            },\n",
        "            \"pred\": {\n",
        "                \"mean\": round(float(np.mean(fk_pred_scores)), 2),\n",
        "                \"std\": round(float(np.std(fk_pred_scores)), 2),\n",
        "                \"median\": round(float(np.median(fk_pred_scores)), 2),\n",
        "            },\n",
        "            \"improvement\": round(float(np.mean(fk_original_scores) - np.mean(fk_pred_scores)), 2),\n",
        "        },\n",
        "\n",
        "        \"bleu_max_over_refs\": {\n",
        "            \"mean\": round(float(np.mean(bleu_scores)), 4),\n",
        "            \"std\": round(float(np.std(bleu_scores)), 4),\n",
        "            \"median\": round(float(np.median(bleu_scores)), 4),\n",
        "        },\n",
        "\n",
        "        \"sari\": {\n",
        "            \"mean\": round(np.mean(sari_scores), 2),\n",
        "            \"std\": round(np.std(sari_scores), 2),\n",
        "            \"median\": round(float(np.median(sari_scores)), 2)\n",
        "        },\n",
        "\n",
        "        \"compression\": {\n",
        "            \"word_ratio\": {\n",
        "                \"mean\": round(float(np.mean(compression_word_ratios)), 4),\n",
        "                \"std\": round(float(np.std(compression_word_ratios)), 4),\n",
        "                \"median\": round(float(np.median(compression_word_ratios)), 4),\n",
        "            },\n",
        "            \"char_ratio\": {\n",
        "                \"mean\": round(float(np.mean(compression_char_ratios)), 4),\n",
        "                \"std\": round(float(np.std(compression_char_ratios)), 4),\n",
        "                \"median\": round(float(np.median(compression_char_ratios)), 4),\n",
        "            },\n",
        "            \"avg_word_reduction_pct\": round((1 - float(np.mean(compression_word_ratios))) * 100, 2),\n",
        "        },\n",
        "\n",
        "        \"avg_sentence_length\": {\n",
        "            \"original\": {\n",
        "                \"mean\": round(float(np.mean(asl_original_scores)), 2),\n",
        "                \"std\": round(float(np.std(asl_original_scores)), 2),\n",
        "                \"median\": round(float(np.median(asl_original_scores)), 2),\n",
        "            },\n",
        "            \"pred\": {\n",
        "                \"mean\": round(float(np.mean(asl_pred_scores)), 2),\n",
        "                \"std\": round(float(np.std(asl_pred_scores)), 2),\n",
        "                \"median\": round(float(np.median(asl_pred_scores)), 2),\n",
        "            },\n",
        "            \"reduction\": round(float(np.mean(asl_original_scores) - np.mean(asl_pred_scores)), 2),\n",
        "        },\n",
        "    }\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "44875068b3f54ac6a731dd2c0494fd81",
            "402d3c5030744d98b26f4ceaf6ff51e4",
            "2cbdbf581336468a9c9fc14a78a53748",
            "9c30c025e47c4fb5887b1bd538546898",
            "b48148504c7c4e8fa86178aa1dd2245d",
            "b9642dde06514d64a7396aa42299efaf",
            "fdf087895105437f9adb44e0583a3d93",
            "cfc78aec3237446684878655e96b1316",
            "3d56c29b8b774c8b91e2fabbb9d9cbed",
            "37bbe35a57a64835b108374638528073",
            "90bfe0b7ad1f4507aff748cab8c371ca",
            "a45d78ec717b4ab099567b5305650759",
            "26960e5d7a454067934497583d05f2ab",
            "7feb53fffd224d95a693a991725f2845",
            "533fcfedbd8249b2843130b12a6cd570",
            "7f14ca1a9ccc480d9f8ddb02510de7b9",
            "19190687a54c4f85ad27c8063c57dc85",
            "6ef3707448d44ae1a3c16648f570c7fa",
            "a8ec9ef326e24365990d4ff7a300cb91",
            "84693675042d416ab6753d13715e17a1",
            "2f707790cbfb4229af44e09dee365b63",
            "03cd65e244cc4e1e9d343180a0f7a55c"
          ]
        },
        "id": "v8yIjazbxfg4",
        "outputId": "cbcec9ca-ec58-4165-d74f-78bbea8fbedb",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating predictions:   0%|          | 0/359 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44875068b3f54ac6a731dd2c0494fd81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Scoring metrics:   0%|          | 0/359 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a45d78ec717b4ab099567b5305650759"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "with open(TEST_JSON, \"r\") as f:\n",
        "    eval_data = json.load(f)\n",
        "\n",
        "'''\n",
        "random.seed(1)\n",
        "\n",
        "n_total = len(eval_data)\n",
        "n_10 = int(0.1 * n_total)\n",
        "\n",
        "eval_data_10 = random.sample(eval_data, n_10)\n",
        "'''\n",
        "\n",
        "best_model = inference_model\n",
        "\n",
        "preds = generate_predictions(eval_data, best_model, tokenizer, device)\n",
        "\n",
        "results = calculate_dataset_metrics(eval_data, preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpY0m8Ds27Zd",
        "outputId": "9c07a79b-b9d0-4d5b-a4f4-072d5adc8e54"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dataset_size': 359,\n",
              " 'flesch_kincaid': {'original': {'mean': 11.8, 'std': 3.92, 'median': 11.68},\n",
              "  'pred': {'mean': 10.05, 'std': 3.85, 'median': 9.62},\n",
              "  'improvement': 1.75},\n",
              " 'bleu_max_over_refs': {'mean': 0.7517, 'std': 0.1515, 'median': 0.7598},\n",
              " 'sari': {'mean': np.float64(45.8), 'std': np.float64(15.09), 'median': 46.71},\n",
              " 'compression': {'word_ratio': {'mean': 0.9331,\n",
              "   'std': 0.095,\n",
              "   'median': 0.9583},\n",
              "  'char_ratio': {'mean': 0.9147, 'std': 0.1002, 'median': 0.9432},\n",
              "  'avg_word_reduction_pct': 6.69},\n",
              " 'avg_sentence_length': {'original': {'mean': 19.37,\n",
              "   'std': 8.01,\n",
              "   'median': 19.0},\n",
              "  'pred': {'mean': 16.18, 'std': 7.37, 'median': 15.0},\n",
              "  'reduction': 3.19}}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "B8bqdvg27ggJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8885337f-9964-4f45-a4b1-e3462404a073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/AML_Final_Project/Results/default_asset_results.json\n"
          ]
        }
      ],
      "source": [
        "# Save results\n",
        "OUT_PATH = \"/content/drive/MyDrive/AML_Final_Project/Results/default_asset_results.json\"\n",
        "with open(OUT_PATH, \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", OUT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J6qRGo_emH63"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}