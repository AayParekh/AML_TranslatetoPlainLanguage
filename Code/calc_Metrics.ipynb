{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "psNd-Rz4aC24"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import cmudict\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('corpora/cmudict')\n",
        "except LookupError:\n",
        "    nltk.download('cmudict')"
      ],
      "metadata": {
        "id": "eFPczDvEaN_L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize CMU Pronouncing Dictionary for syllable counting\n",
        "d = cmudict.dict()\n",
        "\n",
        "def count_syllables(word):\n",
        "    \"\"\"Count syllables in a word using CMU Pronouncing Dictionary.\"\"\"\n",
        "    word = word.lower()\n",
        "    if word in d:\n",
        "        return max([len([y for y in x if y[-1].isdigit()]) for x in d[word]])\n",
        "    else:\n",
        "        # Fallback: estimate syllables by counting vowel groups\n",
        "        word = word.lower()\n",
        "        count = 0\n",
        "        vowels = 'aeiouy'\n",
        "        previous_was_vowel = False\n",
        "        for char in word:\n",
        "            is_vowel = char in vowels\n",
        "            if is_vowel and not previous_was_vowel:\n",
        "                count += 1\n",
        "            previous_was_vowel = is_vowel\n",
        "        # Adjust for silent 'e'\n",
        "        if word.endswith('e'):\n",
        "            count -= 1\n",
        "        if count == 0:\n",
        "            count = 1\n",
        "        return count"
      ],
      "metadata": {
        "id": "rTddV2iXaYV7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flesch_kincaid_grade(text):\n",
        "    \"\"\"\n",
        "    Calculate Flesch-Kincaid Grade Level.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to analyze\n",
        "\n",
        "    Returns:\n",
        "        float: The grade level score\n",
        "    \"\"\"\n",
        "    # Split into sentences\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    # Split into words\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "    if not sentences or not words:\n",
        "        return 0.0\n",
        "\n",
        "    total_sentences = len(sentences)\n",
        "    total_words = len(words)\n",
        "    total_syllables = sum(count_syllables(word) for word in words)\n",
        "\n",
        "    # Flesch-Kincaid Grade Level formula\n",
        "    # https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests\n",
        "    grade = 0.39 * (total_words / total_sentences) + 11.8 * (total_syllables / total_words) - 15.59\n",
        "\n",
        "    return round(grade, 2)"
      ],
      "metadata": {
        "id": "a2bKWKkGaWFx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bleu_score(reference, candidate):\n",
        "    \"\"\"\n",
        "    Calculate BLEU score between reference and candidate text.\n",
        "\n",
        "    BLEU measures how similar the candidate text is to the reference text.\n",
        "    Score ranges from 0 to 1, where 1 indicates perfect match.\n",
        "\n",
        "    Args:\n",
        "        reference (str): The reference (original) text\n",
        "        candidate (str): The candidate (simplified) text\n",
        "\n",
        "    Returns:\n",
        "        float: BLEU score between 0 and 1\n",
        "    \"\"\"\n",
        "    # Tokenize texts into words\n",
        "    reference_tokens = re.findall(r'\\b\\w+\\b', reference.lower())\n",
        "    candidate_tokens = re.findall(r'\\b\\w+\\b', candidate.lower())\n",
        "\n",
        "    # BLEU expects reference as list of lists\n",
        "    reference_list = [reference_tokens]\n",
        "\n",
        "    # Use smoothing to handle cases with no n-gram matches\n",
        "    smoothing = SmoothingFunction().method1\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    score = sentence_bleu(reference_list, candidate_tokens, smoothing_function=smoothing)\n",
        "\n",
        "    return round(score, 4)"
      ],
      "metadata": {
        "id": "rxqllwKSaUNo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sari_score(source, reference, candidate):\n",
        "    \"\"\"\n",
        "    Calculate SARI score for text simplification.\n",
        "\n",
        "    SARI measures the quality of simplification by evaluating:\n",
        "    - Added words (should be simple/appropriate)\n",
        "    - Deleted words (should remove complex content)\n",
        "    - Kept words (should retain important information)\n",
        "\n",
        "    Args:\n",
        "        source (str): The original source text\n",
        "        reference (str): The reference simplified text\n",
        "        candidate (str): The candidate simplified text\n",
        "\n",
        "    Returns:\n",
        "        float: SARI score (0-100 scale)\n",
        "    \"\"\"\n",
        "    # Tokenize texts\n",
        "    source_tokens = set(re.findall(r'\\b\\w+\\b', source.lower()))\n",
        "    reference_tokens = set(re.findall(r'\\b\\w+\\b', reference.lower()))\n",
        "    candidate_tokens = set(re.findall(r'\\b\\w+\\b', candidate.lower()))\n",
        "\n",
        "    # Calculate add, keep, and delete operations\n",
        "    # Add: words in candidate but not in source\n",
        "    added = candidate_tokens - source_tokens\n",
        "    # Keep: words in both source and candidate\n",
        "    kept = source_tokens & candidate_tokens\n",
        "    # Delete: words in source but not in candidate\n",
        "    deleted = source_tokens - candidate_tokens\n",
        "\n",
        "    # Calculate precision and recall for each operation\n",
        "    # Add score: precision of added words (how many added words are in reference)\n",
        "    if added:\n",
        "        add_precision = len(added & reference_tokens) / len(added)\n",
        "    else:\n",
        "        add_precision = 0.0\n",
        "\n",
        "    # Keep score: F1 of kept words\n",
        "    if kept or (source_tokens & reference_tokens):\n",
        "        keep_precision = len(kept & reference_tokens) / len(kept) if kept else 0\n",
        "        keep_recall = len(kept & reference_tokens) / len(source_tokens & reference_tokens) if (source_tokens & reference_tokens) else 0\n",
        "        if keep_precision + keep_recall > 0:\n",
        "            keep_f1 = 2 * keep_precision * keep_recall / (keep_precision + keep_recall)\n",
        "        else:\n",
        "            keep_f1 = 0\n",
        "    else:\n",
        "        keep_f1 = 0\n",
        "\n",
        "    # Delete score: precision of deleted words (how many deleted words are also deleted in reference)\n",
        "    reference_deleted = source_tokens - reference_tokens\n",
        "    if deleted:\n",
        "        delete_precision = len(deleted & reference_deleted) / len(deleted)\n",
        "    else:\n",
        "        delete_precision = 0.0\n",
        "\n",
        "    # SARI is the average of the three scores (scaled to 0-100)\n",
        "    sari = (add_precision + keep_f1 + delete_precision) / 3 * 100\n",
        "\n",
        "    return round(sari, 2)"
      ],
      "metadata": {
        "id": "tHTrmnyCaRvr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compression_ratio(original, simplified):\n",
        "    \"\"\"\n",
        "    Calculate compression ratio between original and simplified text.\n",
        "\n",
        "    Measures how much shorter the simplified text is compared to the original.\n",
        "    Typical good simplifications: 0.6-0.8 (20-40% shorter)\n",
        "\n",
        "    Args:\n",
        "        original (str): The original text\n",
        "        simplified (str): The simplified text\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with character-based and word-based compression ratios\n",
        "    \"\"\"\n",
        "    # Character-based compression\n",
        "    char_ratio = len(simplified) / len(original) if len(original) > 0 else 0\n",
        "\n",
        "    # Word-based compression\n",
        "    original_words = len(re.findall(r'\\b\\w+\\b', original))\n",
        "    simplified_words = len(re.findall(r'\\b\\w+\\b', simplified))\n",
        "    word_ratio = simplified_words / original_words if original_words > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'char_ratio': round(char_ratio, 4),\n",
        "        'word_ratio': round(word_ratio, 4),\n",
        "        'char_reduction_pct': round((1 - char_ratio) * 100, 2),\n",
        "        'word_reduction_pct': round((1 - word_ratio) * 100, 2)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "-UZVnuEGfGkZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_sentence_length(text):\n",
        "    \"\"\"\n",
        "    Calculate average sentence length in words.\n",
        "\n",
        "    Simpler text typically has shorter sentences.\n",
        "    General guidelines:\n",
        "    - <15 words: Very easy\n",
        "    - 15-20 words: Easy\n",
        "    - 20-25 words: Moderate\n",
        "    - >25 words: Difficult\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to analyze\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with average sentence length and total sentences\n",
        "    \"\"\"\n",
        "    # Split into sentences\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    if not sentences:\n",
        "        return {\n",
        "            'avg_sentence_length': 0.0,\n",
        "            'total_sentences': 0\n",
        "        }\n",
        "\n",
        "    # Count words in each sentence\n",
        "    total_words = 0\n",
        "    for sentence in sentences:\n",
        "        words = re.findall(r'\\b\\w+\\b', sentence)\n",
        "        total_words += len(words)\n",
        "\n",
        "    avg_length = total_words / len(sentences)\n",
        "\n",
        "    return {\n",
        "        'avg_sentence_length': round(avg_length, 2),\n",
        "        'total_sentences': len(sentences)\n",
        "    }"
      ],
      "metadata": {
        "id": "E3lyN7U-fI2o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    example =  {\n",
        "    \"original\": \"When a patient arrives at the emergency room, they are first triaged to assess the severity of their condition, after which they are directed to the appropriate treatment area based on priority and available resources.\",\n",
        "    \"simplifications\": [\n",
        "      \"When patients arrive at the emergency room, they are checked to see how serious their condition is and then taken to the right treatment area.\"\n",
        "    ]\n",
        "  }\n",
        "\n",
        "    original = example[\"original\"]\n",
        "    simplified = example[\"simplifications\"][0]\n",
        "\n",
        "    print(\"Original text:\")\n",
        "    print(original)\n",
        "    print(\"\\nSimplified text:\")\n",
        "    print(simplified)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    print(\"\\nMetrics:\")\n",
        "    print(f\"Flesch-Kincaid Grade (Original): {flesch_kincaid_grade(original)}\")\n",
        "    print(f\"Flesch-Kincaid Grade (Simplified): {flesch_kincaid_grade(simplified)}\")\n",
        "\n",
        "    print(f\"\\nBLEU Score: {bleu_score(original, simplified)}\")\n",
        "\n",
        "    # the SARI needs three texts: Source, Reference, and Candidate. So for our project do we not use it?\n",
        "    # print(f\"\\nSARI Score: {sari_score(original, simplified, simplified)}\")\n",
        "\n",
        "    # Compression Ratio\n",
        "    print(\"\\nCompression Ratio:\")\n",
        "    compression = compression_ratio(original, simplified)\n",
        "    print(f\"  Word Ratio: {compression['word_ratio']} ({compression['word_reduction_pct']}% reduction)\")\n",
        "    print(f\"  Char Ratio: {compression['char_ratio']} ({compression['char_reduction_pct']}% reduction)\")\n",
        "\n",
        "    # Average Sentence Length\n",
        "    print(\"\\nAverage Sentence Length:\")\n",
        "    asl_original = average_sentence_length(original)\n",
        "    asl_simplified = average_sentence_length(simplified)\n",
        "    print(f\"  Original: {asl_original['avg_sentence_length']} words/sentence ({asl_original['total_sentences']} sentences)\")\n",
        "    print(f\"  Simplified: {asl_simplified['avg_sentence_length']} words/sentence ({asl_simplified['total_sentences']} sentences)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8LKJW1IaP8R",
        "outputId": "3ee929a4-5061-4a02-e7f3-8da6d128d34f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "When a patient arrives at the emergency room, they are first triaged to assess the severity of their condition, after which they are directed to the appropriate treatment area based on priority and available resources.\n",
            "\n",
            "Simplified text:\n",
            "When patients arrive at the emergency room, they are checked to see how serious their condition is and then taken to the right treatment area.\n",
            "\n",
            "============================================================\n",
            "\n",
            "Metrics:\n",
            "Flesch-Kincaid Grade (Original): 19.64\n",
            "Flesch-Kincaid Grade (Simplified): 12.1\n",
            "\n",
            "BLEU Score: 0.1759\n",
            "\n",
            "Compression Ratio:\n",
            "  Word Ratio: 0.7143 (28.57% reduction)\n",
            "  Char Ratio: 0.6514 (34.86% reduction)\n",
            "\n",
            "Average Sentence Length:\n",
            "  Original: 35.0 words/sentence (1 sentences)\n",
            "  Simplified: 25.0 words/sentence (1 sentences)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculate metrics across an entire dataset of sentence pairs.\n",
        "Assumes you have all the individual metric functions already defined.\n",
        "\"\"\"\n",
        "\n",
        "def calculate_dataset_metrics(data, use_bertscore=True):\n",
        "    \"\"\"\n",
        "    Calculate metrics for an entire dataset of sentence pairs.\n",
        "\n",
        "    Args:\n",
        "        data (list): List of dicts with 'original' and 'simplifications' keys\n",
        "        use_bertscore (bool): Whether to calculate BERTScore (slower)\n",
        "\n",
        "    Returns:\n",
        "        dict: Aggregated metrics across the dataset\n",
        "    \"\"\"\n",
        "    # Storage for individual scores\n",
        "    fk_original_scores = []\n",
        "    fk_simplified_scores = []\n",
        "    bleu_scores = []\n",
        "    compression_word_ratios = []\n",
        "    compression_char_ratios = []\n",
        "    asl_original_scores = []\n",
        "    asl_simplified_scores = []\n",
        "\n",
        "    # Process each example\n",
        "    for item in data:\n",
        "        original = item['original']\n",
        "        simplified = item['simplifications'][0]  # Assuming first simplification\n",
        "\n",
        "        # Flesch-Kincaid\n",
        "        fk_original_scores.append(flesch_kincaid_grade(original))\n",
        "        fk_simplified_scores.append(flesch_kincaid_grade(simplified))\n",
        "\n",
        "        # BLEU\n",
        "        bleu_scores.append(bleu_score(original, simplified))\n",
        "\n",
        "        # Compression\n",
        "        comp = compression_ratio(original, simplified)\n",
        "        compression_word_ratios.append(comp['word_ratio'])\n",
        "        compression_char_ratios.append(comp['char_ratio'])\n",
        "\n",
        "        # Average Sentence Length\n",
        "        asl_orig = average_sentence_length(original)\n",
        "        asl_simp = average_sentence_length(simplified)\n",
        "        asl_original_scores.append(asl_orig['avg_sentence_length'])\n",
        "        asl_simplified_scores.append(asl_simp['avg_sentence_length'])\n",
        "\n",
        "    # Calculate aggregate statistics\n",
        "    results = {\n",
        "        'dataset_size': len(data),\n",
        "        'flesch_kincaid': {\n",
        "            'original': {\n",
        "                'mean': round(np.mean(fk_original_scores), 2),\n",
        "                'std': round(np.std(fk_original_scores), 2),\n",
        "                'median': round(np.median(fk_original_scores), 2)\n",
        "            },\n",
        "            'simplified': {\n",
        "                'mean': round(np.mean(fk_simplified_scores), 2),\n",
        "                'std': round(np.std(fk_simplified_scores), 2),\n",
        "                'median': round(np.median(fk_simplified_scores), 2)\n",
        "            },\n",
        "            'improvement': round(np.mean(fk_original_scores) - np.mean(fk_simplified_scores), 2)\n",
        "        },\n",
        "        'bleu': {\n",
        "            'mean': round(np.mean(bleu_scores), 4),\n",
        "            'std': round(np.std(bleu_scores), 4),\n",
        "            'median': round(np.median(bleu_scores), 4)\n",
        "        },\n",
        "        'compression': {\n",
        "            'word_ratio': {\n",
        "                'mean': round(np.mean(compression_word_ratios), 4),\n",
        "                'std': round(np.std(compression_word_ratios), 4),\n",
        "                'median': round(np.median(compression_word_ratios), 4)\n",
        "            },\n",
        "            'char_ratio': {\n",
        "                'mean': round(np.mean(compression_char_ratios), 4),\n",
        "                'std': round(np.std(compression_char_ratios), 4),\n",
        "                'median': round(np.median(compression_char_ratios), 4)\n",
        "            },\n",
        "            'avg_word_reduction_pct': round((1 - np.mean(compression_word_ratios)) * 100, 2)\n",
        "        },\n",
        "        'avg_sentence_length': {\n",
        "            'original': {\n",
        "                'mean': round(np.mean(asl_original_scores), 2),\n",
        "                'std': round(np.std(asl_original_scores), 2)\n",
        "            },\n",
        "            'simplified': {\n",
        "                'mean': round(np.mean(asl_simplified_scores), 2),\n",
        "                'std': round(np.std(asl_simplified_scores), 2)\n",
        "            },\n",
        "            'reduction': round(np.mean(asl_original_scores) - np.mean(asl_simplified_scores), 2)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "PwQbJTrlhbad"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_results(results):\n",
        "    \"\"\"Pretty print the aggregated results.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"DATASET METRICS (n={results['dataset_size']} pairs)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\nüìä READABILITY (Flesch-Kincaid Grade Level)\")\n",
        "    print(f\"  Original:    {results['flesch_kincaid']['original']['mean']:.2f} ¬± {results['flesch_kincaid']['original']['std']:.2f}\")\n",
        "    print(f\"  Simplified:  {results['flesch_kincaid']['simplified']['mean']:.2f} ¬± {results['flesch_kincaid']['simplified']['std']:.2f}\")\n",
        "    print(f\"  Improvement: {results['flesch_kincaid']['improvement']:.2f} grade levels\")\n",
        "\n",
        "    print(\"\\nüìù SEMANTIC SIMILARITY (BLEU Score)\")\n",
        "    print(f\"  Mean:   {results['bleu']['mean']:.4f} ¬± {results['bleu']['std']:.4f}\")\n",
        "    print(f\"  Median: {results['bleu']['median']:.4f}\")\n",
        "\n",
        "    print(\"\\nüìè COMPRESSION\")\n",
        "    print(f\"  Word Ratio:  {results['compression']['word_ratio']['mean']:.4f} ({results['compression']['avg_word_reduction_pct']:.1f}% reduction)\")\n",
        "    print(f\"  Char Ratio:  {results['compression']['char_ratio']['mean']:.4f}\")\n",
        "\n",
        "    print(\"\\nüìê SENTENCE LENGTH (words/sentence)\")\n",
        "    print(f\"  Original:    {results['avg_sentence_length']['original']['mean']:.2f} ¬± {results['avg_sentence_length']['original']['std']:.2f}\")\n",
        "    print(f\"  Simplified:  {results['avg_sentence_length']['simplified']['mean']:.2f} ¬± {results['avg_sentence_length']['simplified']['std']:.2f}\")\n",
        "    print(f\"  Reduction:   {results['avg_sentence_length']['reduction']:.2f} words\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "V4njqHPuh4Tq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your data from JSON file\n",
        "    with open('/content/drive/MyDrive/AML/AML_Final_Project/Data/synthetic_test.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Calculate metrics (set use_bertscore=False for faster computation)\n",
        "    results = calculate_dataset_metrics(data, use_bertscore=True)\n",
        "\n",
        "    # Print results\n",
        "    print_results(results)\n",
        "\n",
        "    # Optionally save to file\n",
        "    with open('/content/drive/MyDrive/AML/AML_Final_Project/Data/metrics_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(\"Results saved to Google Drive: Shareddrives/AML_Final_Project/metrics_results.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I421Oq5qhdsG",
        "outputId": "16194523-2a54-47b7-b4f4-5d8533e6c159"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "======================================================================\n",
            "DATASET METRICS (n=4470 pairs)\n",
            "======================================================================\n",
            "\n",
            "üìä READABILITY (Flesch-Kincaid Grade Level)\n",
            "  Original:    21.85 ¬± 2.84\n",
            "  Simplified:  11.95 ¬± 3.10\n",
            "  Improvement: 9.90 grade levels\n",
            "\n",
            "üìù SEMANTIC SIMILARITY (BLEU Score)\n",
            "  Mean:   0.0498 ¬± 0.0548\n",
            "  Median: 0.0260\n",
            "\n",
            "üìè COMPRESSION\n",
            "  Word Ratio:  0.5916 (40.8% reduction)\n",
            "  Char Ratio:  0.5208\n",
            "\n",
            "üìê SENTENCE LENGTH (words/sentence)\n",
            "  Original:    31.32 ¬± 5.12\n",
            "  Simplified:  18.35 ¬± 4.44\n",
            "  Reduction:   12.96 words\n",
            "\n",
            "Results saved to Google Drive: Shareddrives/AML_Final_Project/metrics_results.json\n"
          ]
        }
      ]
    }
  ]
}