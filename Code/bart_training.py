# -*- coding: utf-8 -*-
"""BART_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WXWG7QuOaNIe5_WCTv9gh3_tf1JT1F35
"""

import os

os.getcwd()

from google.colab import drive
drive.mount('/content/drive')

os.makedirs("/content/drive/My Drive/colab_data", exist_ok=True)

!pip install -q "transformers>=4.45.0" "datasets>=2.20.0" accelerate sentencepiece

import transformers, datasets
print("Transformers version:", transformers.__version__)
print("Datasets version:", datasets.__version__)

# ============================
# 0. INSTALLS (run once per runtime)
# ============================
# If you haven't already:
# !pip install -q "transformers>=4.30.0" "datasets>=2.14.0" accelerate sentencepiece

# ============================
# 1. IMPORTS & CONFIG
# ============================

import os
from typing import Dict, Any

import torch
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
)

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Optional: nicer CUDA error messages
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

# ---- paths to your JSON files in Drive ----
# Adjust these if your folder is different
TRAIN_JSON = "/content/drive/My Drive/colab_data/asset_train.json"
TEST_JSON  = "/content/drive/My Drive/colab_data/asset_test.json"

MODEL_NAME = "facebook/bart-base"
OUTPUT_DIR = "bart_plain_language_model_json_manual"

# Short operational "prompt" used during training and inference
TASK_PREFIX = (
    "Explain this in simple, plain language for a general audience. "
    "Use short sentences and everyday words, but keep all important information.\n\n"
)

# Max lengths â€“ your examples are short
MAX_SOURCE_LENGTH = 128   # input: prefix + original
MAX_TARGET_LENGTH = 64    # output: simplified

# Training hyperparameters
NUM_EPOCHS = 2          # good compromise between time and quality
BATCH_SIZE = 8          # if you hit OOM, drop to 4
LEARNING_RATE = 5e-5
GRAD_CLIP = 1.0

# ============================
# 2. LOAD JSON DATA
# ============================

# Expecting JSON structure:
# [
#   {
#     "original": " ... ",
#     "simplifications": ["s1", "s2", ...]
#   },
#   ...
# ]

raw_datasets = load_dataset(
    "json",
    data_files={
        "train": TRAIN_JSON,
        "eval": TEST_JSON,
    },
)

train_dataset = raw_datasets["train"]
eval_dataset = raw_datasets["eval"]

print("Raw train entries:", len(train_dataset))
print("Raw eval entries:", len(eval_dataset))

# ============================
# 3. FLATTEN MULTIPLE SIMPLIFICATIONS
# ============================

# Turn:
#   original = "..."
#   simplifications = ["a", "b", "c"]
# into:
#   (original, "a"), (original, "b"), (original, "c")

def explode_simplifications(examples: Dict[str, Any]) -> Dict[str, Any]:
    new_originals = []
    new_simplified = []

    for orig, sims in zip(examples["original"], examples["simplifications"]):
        for s in sims:
            new_originals.append(orig)
            new_simplified.append(s)

    return {
        "original": new_originals,
        "simplified": new_simplified,
    }

train_flat = train_dataset.map(
    explode_simplifications,
    batched=True,
    remove_columns=train_dataset.column_names,
)

eval_flat = eval_dataset.map(
    explode_simplifications,
    batched=True,
    remove_columns=eval_dataset.column_names,
)

MAX_TRAIN_SAMPLES = 5000   # or even 2000 if you want super fast
MAX_EVAL_SAMPLES = 1000

train_flat = train_flat.select(range(min(MAX_TRAIN_SAMPLES, len(train_flat))))
eval_flat = eval_flat.select(range(min(MAX_EVAL_SAMPLES, len(eval_flat))))

print("Subset train examples:", len(train_flat))
print("Subset eval examples:", len(eval_flat))

# ============================
# 4. TOKENIZER & MODEL
# ============================

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
model.to(device)

# ============================
# 5. PREPROCESSING FUNCTIONS
# ============================

def preprocess_function(examples: Dict[str, Any]) -> Dict[str, Any]:
    # Build source strings with the task prefix + original text
    sources = [
        TASK_PREFIX + text.strip()
        for text in examples["original"]
    ]
    targets = [text.strip() for text in examples["simplified"]]

    # Tokenize inputs
    model_inputs = tokenizer(
        sources,
        max_length=MAX_SOURCE_LENGTH,
        truncation=True,
    )

    # Tokenize targets as "labels"
    # (using as_target_tokenizer even though it's deprecated; it works in your env)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets,
            max_length=MAX_TARGET_LENGTH,
            truncation=True,
        )["input_ids"]

    model_inputs["labels"] = labels
    return model_inputs

tokenized_train = train_flat.map(
    preprocess_function,
    batched=True,
    remove_columns=train_flat.column_names,
)

tokenized_eval = eval_flat.map(
    preprocess_function,
    batched=True,
    remove_columns=eval_flat.column_names,
)

print("Tokenized train sample keys:", tokenized_train[0].keys())

# ============================
# 6. DATALOADERS
# ============================

data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
)

train_dataloader = DataLoader(
    tokenized_train,
    shuffle=True,
    batch_size=BATCH_SIZE,
    collate_fn=data_collator,
)

eval_dataloader = DataLoader(
    tokenized_eval,
    shuffle=False,
    batch_size=BATCH_SIZE,
    collate_fn=data_collator,
)

# ============================
# 7. OPTIMIZER (PyTorch AdamW, not transformers.AdamW)
# ============================

optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)

# ============================
# 8. TRAINING LOOP
# ============================

for epoch in range(NUM_EPOCHS):
    print(f"\n==== Epoch {epoch + 1}/{NUM_EPOCHS} ====")
    model.train()
    total_train_loss = 0.0

    for step, batch in enumerate(train_dataloader):
        batch = {k: v.to(device) for k, v in batch.items()}

        outputs = model(**batch)
        loss = outputs.loss

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)
        optimizer.step()
        optimizer.zero_grad()

        total_train_loss += loss.item()

        # progress log
        if (step + 1) % 100 == 0:
            print(f"  Step {step + 1}/{len(train_dataloader)}, "
                  f"batch loss: {loss.item():.4f}")

    avg_train_loss = total_train_loss / max(1, len(train_dataloader))
    print(f"Average training loss: {avg_train_loss:.4f}")

    # ---- EVAL ----
    model.eval()
    total_eval_loss = 0.0

    with torch.no_grad():
        for batch in eval_dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            total_eval_loss += loss.item()

    avg_eval_loss = total_eval_loss / max(1, len(eval_dataloader))
    print(f"Average eval loss: {avg_eval_loss:.4f}")
    # ---- EVAL ----
    model.eval()
    total_eval_loss = 0.0

    with torch.no_grad():
        for batch in eval_dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            total_eval_loss += loss.item()

    avg_eval_loss = total_eval_loss / max(1, len(eval_dataloader))
    print(f"Average eval loss: {avg_eval_loss:.4f}")

# ============================
# 9. SAVE MODEL
# ============================

os.makedirs(OUTPUT_DIR, exist_ok=True)
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print("Training complete. Model saved to:", OUTPUT_DIR)

# ============================
# 10. INFERENCE HELPER
# ============================

inference_tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)
inference_model = AutoModelForSeq2SeqLM.from_pretrained(OUTPUT_DIR).to(device)
inference_model.eval()

def simplify_trained(
    text: str,
    max_new_tokens: int = 64,
    num_beams: int = 4,
) -> str:
    """
    Use the fine-tuned model to simplify new text.
    The only *input* from the user is `text`.
    The TASK_PREFIX is internal and consistently applied.
    """
    model_input = TASK_PREFIX + text.strip()

    enc = inference_tokenizer(
        model_input,
        return_tensors="pt",
        truncation=True,
        max_length=MAX_SOURCE_LENGTH,
    ).to(device)

    with torch.no_grad():
        output_ids = inference_model.generate(
            **enc,
            max_new_tokens=max_new_tokens,
            num_beams=num_beams,
            early_stopping=True,
        )

    return inference_tokenizer.decode(output_ids[0], skip_special_tokens=True)

# ============================
# 11. QUICK TEST ON A NEW TEXT
# ============================

test_text = (
    "Adjacent counties are Marin (to the south), Mendocino (to the north), "
    "Lake (northeast), Napa (to the east), and Solano and Contra Costa (to the southeast)."
)

print("\n=== ORIGINAL ===\n")
print(test_text)

print("\n=== SIMPLIFIED (MODEL OUTPUT) ===\n")
print(simplify_trained(test_text))

import os, glob

# If using Drive:
from google.colab import drive
drive.mount('/content/drive')

print("In /content:")
print(glob.glob("/content/*.json"))

print("\nIn /content/drive/My Drive:")
print(glob.glob("/content/drive/My Drive/**/*.json", recursive=True))